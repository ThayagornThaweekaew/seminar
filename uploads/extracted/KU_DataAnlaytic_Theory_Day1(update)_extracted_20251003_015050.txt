DAY 1:
DATA ANALYTICS 
AND PREDICTION 
MODEL WITH 
PYTHON

AGENDA 
Day 1 : Morning Time
•Introduction to data analytics, BI and Data Science
•Introduction to data analytics
•Transforming Data
•Data Analysis and Presentation
2

INTRODUCTION TO 
DATA ANALYTICS, BI 
AND DATA SCIENCE

DATA ANALYTICS, BUSINESS 
INTELLIGENCE (BI) AND  DATA SCIENCE
4Data Science Business Intelligence –BI Data Analytics หัวข้อ
ศาสตร์ทีÉผสมผสานระหว่างสถิติ คณิตศาสตร์ การเขียน
โปรแกรม และความรู้ด้านธุรกิจ เพืÉอนําข้อมูลมาใช้สร้าง
แบบจําลองเชิงพยากรณ์ (Predictive) และเชิงแนะนํา 
(Prescriptive)ชุดของเครืÉองมือและระบบทีÉรวบรวม จัดเก็บ และนําเสนอ
ข้อมูลธุรกิจ เพืÉอช่วยผู้บริหารและผู้ใช้งานในการติดตามผล
การดําเนินงานและตัดสินใจได้รวดเร็วกระบวนการวิเคราะห์ข้อมูลทัÊงในอดีตและปัจจุบัน 
เพืÉอหาข้อสรุป แนวโน้ม และสาเหตุของเหตุการณ์
ทีÉเกิดขึÊน ช่วยให้องค์กรเข้าใจ “เกิดอะไรขึÊน” และ 
“ทําไมถึงเกิดขึÊน”ความหมาย
•ใช้ Machine Learning (ML) และ Artificial 
Intelligence (AI)
•ตอบคําถามว่า “จะเกิดอะไรขึÊน?” และ “ควรทําอย่างไร?”
•ต้องการทักษะการเขียนโปรแกรม (Python, R) และการ
จัดการ Big Data•ใช้ Dashboard, KPI, และรายงานต่าง ๆ
•เน้นการนําเสนอข้อมูลทีÉใช้งานง่าย เข้าใจได้ทันที
•ส่วนใหญ่เป็นการวิเคราะห์เชิงพรรณนา (Descriptive) 
และวิเคราะห์สาเหตุ (Diagnostic)•มุ่งเน้นการใช้สถิติและการแสดงผลข้อมูล 
(Data Visualization)
•ทํางานกับข้อมูลย้อนหลังและข้อมูลปัจจุบัน
•เป็นพืÊนฐานในการตัดสินใจเชิงธุรกิจลักษณะเด่น
•แบบจําลองทํานายยอดขายล่วงหน้า
•ระบบแนะนําสินค้า (Recommendation System) เช่น 
Amazon, Netflix
•การตรวจจับธุรกรรมทุจริต (Fraud Detection)•Dashboard แสดงยอดขายแบบ Real-time
•รายงานสรุปผลกําไรขาดทุนรายเดือนของบริษัท•วิเคราะห์ยอดขายย้อนหลังเพืÉอหาสาเหตุการ
ตกตํÉา
•วิเคราะห์พฤติกรรมการยกเลิกบริการของ
ลูกค้า (Customer Churn )ตัวอย่าง

DATA ANALYTICS, BUSINESS 
INTELLIGENCE (BI) AND  DATA SCIENCE
5เครืÉองมือ ผู้ใช้งานหลัก ประเภทการวิเคราะห์ * คําถามหลัก จุดเน้น ระดับ
Excel, SQL, 
VisualizationData Scientist, 
Data AnalystDescriptive, Diagnostic, เกิดอะไรขึÊน? ทําไม?วิเคราะห์ข้อมูลในอดีต/ปัจจุบัน มีผลลัพธ์ 
Insight ลึก, แบบจําลอง, การคาดการณ์Data Analytics
ใช้ข้อมูลเพืÉอหาสาเหตุและอธิ บาย
อดีต
Power BI, Tableau, 
SAP Analytics Cloudผู้บริหาร, Business 
UsersDescriptive, Diagnostic ตอนนีÊเกิดอะไรขึÊน?การติดตามผลธุรกิจแบบ Real-time มี
ผลลัพธ์
Dashboard, KPI, รายงานBusiness Intelligence (BI)
ใช้เครืÉองมือสร้างDashboard/
รายงานเพืÉอช่วยตัดสินใจ
Python, R, ML/AIData Scientist, 
Data Engineer, 
ML Engineer, 
Business AnalystDescriptive, Diagnostic, 
Predictive, PrescriptiveจะเกิดอะไรขึÊน? ควรทํา
อะไร?การทํานายและการแนะนําแนวทางมี
ผลลัพธ์เป็น การคาดการณ์, ระบบ
อัตโนมัติ, การเพิÉมประสิทธิภาพ, การ
สนับสนุนการตัดสินใจData Science
ใช้ ML/AI เพืÉอทํานายและเสนอ
แนวทางอนาคต
ประเภทการวิเคราะห์ 
1.Descriptive Analytics –อธิบายว่า “เกิดอะไรขึÊน?”
2.Diagnostic Analytics –อธิบายว่า “ทําไมถึงเกิดขึÊน?”
3.Predictive Analytics –ทํานายว่า “จะเกิดอะไรขึÊนต่อไป?
4.Prescriptive Analytics –แนะนําว่า “ควรทําอย่างไร?”

USE CASES
6Data Science Business Intelligence (BI) Data Analytics หมวด
มุ่งเน้นการพยากรณ์และการแนะนําแนวทาง 
(Predictive & Prescriptive)มุ่งเน้นการติดตามผลธุรกิจแบบ Real-
time และช่วยตัดสินใจรวดเร็วมุ่งเน้นการทําความเข้าใจสิÉงทีÉ
เกิดขึÊนและสาเหตุ
Fraud Detection, Credit Scoring Dashboard งบการเงิน วิเคราะห์ต้นทุน, กําไร Finance
Recommendation Engine, Customer 
SegmentationDashboard แสดงยอดขาย/
การตลาดวิเคราะห์ ROI ของ
แคมเปญMarketing
Predictive Attrition Model Dashboard แสดงจํานวนพนักงาน วิเคราะห์การลาออก HR
Demand Forecasting, Route 
OptimizationDashboard Stock/Delivery วิเคราะห์สาเหตุ delay Supply Chain
Predictive Maintenance Dashboard สรุปผลผลิต วิเคราะห์ defect rate Manufacturing

COMPARE BETWEEN DATA ANALYTICS &BI
1. Data Analytics Example
เน้น: ค้นหาสาเหตุ รูปแบบ แนวโน้ม และการพยากรณ์
สถานการณ์: บริษัทอีคอมเมิร์ซอยากรู้ว่ายอดขายไตรมาสทีÉแล้วทําไมถึงลดลง
ขัÊนตอน Data Analytics
•เก็บข้อมูลดิบ: ทราฟฟิกเว็บไซต์, ข้อมูลลูกค้า, แคมเปญโฆษณา, รายการ
ขาย
•ใช้เครืÉองมือวิเคราะห์ (Python, R, SQL)
•ทําการพยากรณ์ (เช่น Regression) เพืÉอดูว่า การใช้จ่ายด้านการตลาด, 
การตัÊงราคา, หรือปัญหาเว็บ มีผลกับยอดขายอย่างไร
•แบ่งกลุ่มลูกค้า (Customer Segmentation) เช่น Cluster เพืÉอหาลูกค้า
กลุ่มสําคัญ
ผลลัพธ์
•“ลูกค้าวัย 25–34 ปี ทีÉมาจากโฆษณา Instagram มีอัตราการซืÊอสูงกว่า 
40%”
•“การทีÉเว็บไซต์ล่มทําให้ยอดขายลดลง 10%”2. Business Intelligence (BI) Example
เน้น: การติดตาม รายงาน แดชบอร์ด เพืÉอใช้ตัดสินใจ
สถานการณ์: บริษัทอีคอมเมิร์ซอยากให้ผู้จัดการสามารถติดตามยอดขายประจําวันได้
ขัÊนตอน BI
•เชืÉอม Data Warehouse กับเครืÉองมือ BI (Power BI, Tableau, SAP Analytics 
Cloud)
•สร้าง Dashboard แสดง KPI: ยอดขายตามสินค้า, ยอดขายเทียบเป้า, ลูกค้า Top 
10, รายได้ตามภูมิภาค, สต็อกสินค้า
•จัดทํารายงานอัตโนมัติส่งผู้จัดการทุกวันจันทร์
ผลลัพธ์
•Dashboard แสดงว่า: “ยอดขายสัปดาห์ทีÉแล้ว = 1.2M บาท (ตํÉากว่าเป้า 5%) สินค้า
ขายดีสุด: หูฟังไร้สาย”
•สามารถเจาะดูรายละเอียดยอดขายตามภูมิภาค สินค้า หรือช่องทางการตลาด
•แจ้งเตือน: “สินค้ารหัส X เหลือน้อยกว่ากําหนดในสต็อก”
7

แนวโน้มสําคัญใน DATA ANALYTICS
8การวิเคราะห์แบบเรียลไทม์ (Real-time Analytics)
การใช้ข้อมูลและตอบสนองในทันที เช่น Snowpipeโหลดข้อมูลเข้า Snowflake Data Warehouse แบบอัตโนมัติและเกือบ Real-time โดยไม่ต้องรัน Batch 
Load แบบ Manual เมืÉอมีไฟล์ใหม่ เช่น CSV, JSON
Data Mesh & Decentralization
การกระจายความรับผิดชอบด้านข้อมูลสู่แต่ละหน่วยงาน ช่วยให้การเข้าถึงและวิเคราะห์ข้อมูลเร็วขึÊนและครอบคลุมกว่าเดิม
ตัวอย่างการใช้งานจริงก่อนใช้ Data Mesh (Centralized) ทีม Marketing ขอ Data จาก Data Lake ส่วนกลาง ทําหให้ต้องรอ Data Engineer รวมข้อมูล
จากหลายระบบทําให้ใช้เวลานานหลังใช้ Data Mesh (Decentralized) ทีม Marketing มี Data Owner + Pipeline ของตนเอง ทําให้ สร้าง Data Product 
“Campaign Performance”  และทีม Finance สามารถ reuse Data Product นีÊได้โดยตรงผ่าน data catalog โดยไม่ต้องทํา integration เอง
NLP และ Natural Language Interfaces
เทคโนโลยีทีÉทําให้คอมพิวเตอร์เข้าใจภาษามนุษย์เช่น Google Translate, CHATGBT, ระบบตรวจสอบไวยากรณ์
ความสดใหม่ & คุณภาพของข้อมูล (Veracity & Data Observability)
การตรวจสอบและควบคุมคุณภาพข้อมูลเพืÉอให้มัÉนใจว่าข้อมูลทีÉใช้วิเคราะห์ถูกต้องและทันสมัย
GenAI & RAG (Retrieval-Augmented Generation)
สามารถ “สร้าง” ข้อมูลใหม่จากโมเดล Machine Learning ทีÉเรียนรู้ pattern ของข้อมูลเดิมเช่น ChatGPT →สร้างข้อความ, โค้ด, บทความ co-pilot 
สร้าง code

แนวโน้มสําคัญใน BUSINESS INTELLIGENCE (BI)
9Augmented Analytics
การนํา AI และ Machine Learning มาอัตโนมัติในงานจัดเตรียมข้อมูล การวิเคราะห์ และการสร้างความเข้าใจ ได้แก่ การใช้ Natural Language Query (NLQ)  เช่น 
SAP Analytics Cloud: ผู้ใช้พิมพ์ “ยอดขายปีนีÊเทียบกับปีทีÉแล้ว จะได้กราฟออกมา หรือ Power BI Q&A: ผู้ใช้พิมพ์ “Top 5 customers by revenue” ระบบสร้าง
รายงานอัตโนมัติ
Semantic Layer & MCP (Metric Context Protocol)
คือ ชัÊนกลาง (abstraction layer) ระหว่าง Data Source (Database, Data Lake, Warehouse) และ Business User/BI Tool ทําหน้าทีÉแปลง raw data ให้อยู่ใน
รูปแบบทีÉ “เข้าใจตรงกัน” และ “สืÉอสารด้วยภาษาธุรกิจ”ช่วยให้ผู้ใช้ไม่ต้องเขียน SQL เอง  และ MCP เป็น protocol ทีÉทําให้ semantic layers จากหลายระบบ คุยกัน
ได้
Self-Service BI & Democratization
ผู้ใช้งานธุรกิจสามารถสร้างรายงานได้เอง โดยไม่ต้องพึÉงทีม IT, เพิÉมทักษะและการเข้าถึงข้อมูลอย่างทัÉวถึง เช่น การใช้ drag &drop /low code no code
Data Governance, Security & Quality
การรักษาความน่าเชืÉอถือของข้อมูล การดูแลคุณภาพความสมบูรณ์ของข้อมูล และการจัดการความปลอดภัยเพืÉอเสริมให้ข้อมูลจาก BI มีความน่าเชืÉอถือ
BI-as-a-Service & Collaborative BI
บริษัทขนาดกลางทีÉใช้ BI-as-a-Service (Cloud) + Embedded BI  โดยBI ไม่ได้แยกเป็น platform เดียว แต่ฝังอยู่ใน ระบบงานหลัก (Operational Systems) เช่น 
ERP, CRM, HRM → ผู้ใช้เห็น dashboard/analytics อยู่ใน workflow เดิม → ไม่ต้องสลับหน้าจอช่วยให้ข้อมูลเป็นส่วนหนึÉงของการตัดสินใจประจําวัน ไม่ใช่
แค่รายงานทีÉดูย้อนหลัง•”

แนวโน้มสําคัญใน DATA SCIENCE
10จัดการกับ “Dark Data”
Dark Data = “ ข้อมูลทีÉเก็บไว้แต่ไม่ได้ใช้” เช่น อีเมล เอกสาร → ถ้าไม่จัดการจะกลายเป็น ต้นทุน + ความเสีÉยง
การจัดการ = ค้นหา → ประเมิน → จัดการ (เก็บ/ลบ/ย้าย) → สร้าง governance →หาประโยชน์ถ้าเป็นไปได้
สร้างมูลค่าด้วย AI, Knowledge Graph และระบบอัจฉริยะ
ลงทุนในโครงสร้างข้อมูลทีÉเชืÉอถือได้ก่อนใช้ AI
การพัฒนาคุณภาพข้อมูลและระบบทีÉมัÉนคงเป็นพืÊนฐานก่อนเริÉมใช้ AI โดยเฉพาะองค์กรใหญ่ทีÉเก่งในด้าน Governance ได้ผลดีกว่ามาก
GenAI จากแค่ช่วยเรืÉองความเร็ว →เข้าถึงการตัดสินใจธุรกิจ
AI ไม่ใช่แค่ช่วยทํางานเร็ว แต่เริÉมถูกใช้วิเคราะห์ตลาด, ทํา M&A insights และช่วยควบคุมความเสี Éยงอย่างมีประสิทธิภาพ

INTRODUCTION TO 
DATA ANALYTICS

BACKGROUND
ข้อมูล (Data) มีการเพิÉมพูนอย่างก้าวกระโดดในอัตราทวีคูณ (Exponential Growth) ปรากฏการณ์การ
แพร่กระจายของข้อมูล (Data Proliferation) นีÊเกิดจากการพัฒนาของพลังการประมวลผลของคอมพิวเตอร์ 
(Computer Processing Power) ทีÉเพิÉมสูงขึÊน ความสามารถในการจัดเก็บข้อมูล (Storage Capacity) ทีÉ
ขยายตัวอย่างต่อเนืÉอง ตลอดจนแบนด์วิดท์ (Bandwidth) ทีÉมีศักยภาพมากขึÊน ซึÉงจนถึงปัจจุบันยังไม่
ปรากฏสัญญาณของการชะลอตัวลงแต่อย่างใด
ภายใต้บริบทของโลกอินเทอร์เน็ตในทุกสรรพสิÉง (Internet of Things: IoT) เมืÉอผนวกเข้ากับเทคโนโลยี
แบนด์วิดท์ยุคทีÉห้า (5G Bandwidth) ปริมาณข้อมูลทีÉถูกสร้างขึÊนและส่งต่อมาจากแหล่ งกําเนิด (Data 
Sources) ทีÉหลากหลายจะยิ ÉงทวีจํานวนมากขึÊนอย่างมีนัยสําคัญ
คําถามเชิงวิชาการทีÉสําคัญจึงเกิดขึÊนว่า ข้อมูลจํานวนมหาศาลเหล่านีÊมีคุณค่าในเชิงใด และองค์กรธุรกิจ
สามารถใช้ข้อมูลดังกล่าวเพืÉอสร้างองค์ความรู้เชิงลึก (Insights) และเสริมสร้างความได้เปรียบทางการ
แข่งขัน (Competitive Advantage) ได้หรือไม่ บทนีÊจึงมุ่งศึกษาแนวคิดว่าด้วยการวิเคราะห์ข้อมูล (Data 
Analytics) และเครืÉองมือทีÉเกีÉยวข้อง (Toolsets) ซึÉงมีความจําเป็นต่อการเปลีÉยนข้อมูลปริมาณมหาศาลให้
กลายเป็นสารสนเทศ ( Information) ทีÉมีคุณค่าและสามารถนําไปใช้ประโยชน์ได้จริง
12

13BACKGROUND
การใช้ข้อมูล (Data) อย่างเหมาะสมได้กลายเป็นประเด็นทีÉมีความสําคัญอย่างยิÉงสําหรับวิชาชีพบัญชี นักบัญชีในทุกสาขาการปฏิบัติงานต่างก็นําข้อมูลมาใช้ในรูปแบบทีÉ
น่าสนใจและหลากหลาย ตัวอย่างเช่น
ประโยชน์ทีÉได้รับ (Benefits) การใช้ข้อมูล (Data Usage) สายงาน (Practice Area)
- เพิÉมคุณภาพและความน่าเชื Éอถือของการตรวจสอบ - ลดความ
เสีÉยงจากการพึÉงพาตัวอย่างขนาดเล็ก - สนับสนุนการปฏิบัติตาม
กฎระเบียบอย่างมีประสิทธิภาพ- ใช้การวิเคราะห์ข้อมูล (Data Analytics) และระบบอัตโนมัติ (Automation) 
เพืÉอตรวจสอบธุรกรรมทั Êงประชากร (Full Population) แทนการสุ่มตัวอย่าง 
(Sample) -จัดหาหลักฐานทีÉชัดเจนมากขึÊนในการพิสูจน์การปฏิบัติตามมาตรการ
ทางการบัญชี (Accounting Rules)ผู้สอบบัญชี (Auditors –Internal &
External)
- การตัดสินใจด้านต้นทุนและการกําหนดราคาแม่นยําขึÊน - ลด
ความเสีÉยงทางการเงิน - เพิÉมศักยภาพในการสร้างและรักษามูลค่า
องค์กร- ใช้ข้อมูลเพืÉอคํานวณต้นทุนสินค้าและบริการ (Costing Products & Services) 
ได้อย่างแม่นยํา - ประเมินความเสีÉยง (Risk Assessment) ได้ดีขึÊน - ระบุโอกาส
ในการรักษาและเพิÉมมูลค่า (Preserve & Enhance Value)นักบัญชีฝ่ายองค์กร (Corporate 
Accountants)
- เพิÉมความสามารถในการวางแผนภาษีเชิงกลยุทธ์ - ลดความ
เสีÉยงจากข้อพิพาททางภาษี - มีบทบาทเชิงรุกต่อการตัดสินใจทาง
ธุรกิจระดับสูง- ใช้ Data Analytics เพืÉอประมาณการผลกระทบทางภาษี (Tax 
Consequences) แบบเรียลไทม์ (Real-time Estimates) - ตอบสนองต่อการ
ตรวจสอบจากหน่วยงานกํากับดูแล (Regulators) ได้มีประสิทธิภาพมากขึÊนผู้เชีÉยวชาญด้านภาษี (Tax 
Professionals)
- เพิÉมความแม่นยําในการคัดเลือกการลงทุน - สร้างความเชืÉอมัÉน
กับลูกค้า - เพิÉมความสามารถในการแข่งขันในตลาดการเงิน- ใช้ข้อมูลเพืÉอระบุโอกาสการลงทุนทีÉให้ผลตอบแทนทีÉดีกว่า (Favorable 
Investment Opportunities) และนํามาแนะนําแก่ลูกค้าทีÉปรึกษาการลงทุน (Investment 
Advisors)

14BACKGROUND
จากตัวอย่างทีÉกล่าวมา แสดงให้เห็นว่าผู้ใช้งาน จําเป็นต้องมีความเข้าใจเกีÉยวกับข้อมูล (Data) และการเปลีÉยนแปลงทีÉเกิดขึÊนกับโลกธุรกิจ เพืÉอทําความเข้าใจขอบเขต
ของการปฏิวัติข้อมูล (Data Revolution) สิÉงสําคัญคือการพิจารณาแนวคิด “4 V’s of Big Data” ได้แก่ Volume, Velocity, Variety และ Veracity

15ANALYTIC MINDSET
กรอบความคิด (Mindset) หมายถึง ทัศนคติทางจิตใจ วิธีคิด หรือกรอบความคิดทีÉบุคคลใช้ในการรับรู้และตัดสินใจ กรอบความคิดเป็นการรวบรวมของความเชืÉอและ
ความคิดทีÉทรงพลัง ซึÉงมีอิทธิพลต่อการคิด ความรู้สึก และการกระทําของบุคคล  
ตารางนีÊช่วยให้เห็นชัดว่า Analytics Mindset ในทางบัญชี มีรากฐานคล้ายกับ Scientific Method ทีÉใช้ในงานวิจัยวิทยาศาสตร์
1
2
3
4

1. ASK THE RIGHT QUESTION
คํานิยามของข้อมูล(Data) ว่าเป็นข้อเท็จจริงทีÉถูกรวบรวม(Collected) บันทึก
(Recorded) จัดเก็บ(Stored) และประมวลผล (Processed) โดยระบบดังนัÊนข้อมูล
เพียงลําพังยังมีคุณค่าไม่มากนักแต่เมืÉอข้อมูลถูกแปลงเป็นสารสนเทศ(Information) 
จึงจะสร้างคุณค่าได้
การเริÉมต้นกระบวนการเปลีÉยนข้อมูลให้เป็นสารสนเทศจําเป็นต้องมีคําถาม
(Question) หรือผลลัพธ์ทีÉต้องการ(Desired Outcome) การตัÊงคําถามทีÉถูกต้อง
(Asking the Right Question) ถือเป็นก้าวแรกของกรอบความคิดเชิงการวิเคราะห์
(Analytics Mindset) เพืÉอกําหนดว่า“คําถามทีÉดี” (Good Question) หรือ“คําถามทีÉ
ถูกต้อง” (Right Question) ในบริบทของการวิเคราะห์ข้อมูล(Data Analytics) เป็น
อย่างไรคําถามควรถูกออกแบบตามหลักการSMART ได้แก่:
16ชัดเจน วัดได้ มีเป้าหมาย
แบบฉลาด (Smart Question) แบบทัÉวไป (ไม่ชัด)
ยอดขาย Q2/2025 ของสินค้ากลุ่ม SUV ในไทย เพิÉม/ลดกีÉ % เมืÉอเทียบกับ Q2/2024? ยอดขายเราเป็นยังไง?
อัตราการ Churn ของลูกค้า SME ในกลุ่ม Telecom ช่วง H1/2025 เท่าไร และสาเหตุหลักคืออะไร? ลูกค้าลดลงไหม?
ต้นทุนต่อหน่วย (Unit Cost) ของ Product A ในโรงงานเชียงใหม่ Q1/2025 สูงกว่ามาตรฐานทีÉตัÊงไว้ (120 บาท/หน่วย) หรือไม่? ต้นทุนสูงไหม?
ค่า Employee Engagement Score ปี 2025 ในฝ่าย IT เทียบกับปี 2024 แตกต่างกันอย่างไร? พนักงานแฮปปีÊไหม?ตัวอย่าง

2. EXTRACT, TRANSFORM, AND LOAD RELEVANT DATA
กระบวนการ ETL หรือการ ดึง (Extract), แปลง (Transform) และ บรรทุก (Load) ข้อมูล ถือเป็นขัÊนตอน
ทีÉใช้เวลามากทีÉสุดของการสร้าง Analytics Mindset เนืÉองจากกระบวนการนี Êแตกต่างกันไปตามแต่ละ
โปรแกรม ฐานข้อมูล หรือระบบทีÉจัดเก็บและใช้งานข้อมูล ทําให้ต้องมีการส่งออก (Export) แปลง 
(Transform) และบรรทุก (Load) เข้าสู่ระบบใหม่ซํÊาหลายครัÊง
17การดึงข้อมูล (Extracting Data)
ขัÊนแรกของ ETL คือ การดึงข้อมูล (Extraction) ซึÉงแบ่งเป็น 3 ขัÊนตอนหลัก:
1.เข้าใจความต้องการของข้อมูลและข้อมูลทีÉมีอยู่ (Understand Data Needs and Available Data)
•ต้องกําหนดความต้องการข้อมูลอย่างชัดเจน (สัมพันธ์กับ Analytics Mindset ข้อแรกคือ Asking the Right 
Question) หากกําหนดไม่ดี อาจดึงข้อมูลผิดหรือไม่ครบ ต้องทํา ETL ซํÊา → เสียเวลาและทรัพยากร
2.ดําเนินการดึงข้อมูล (Perform the Data Extraction)
•ต้องเข้าใจแหล่งทีÉอยู่ของข้อมูล (Location) การเข้าถึง (Accessibility) และโครงสร้าง (Structure)
•องค์กรนิยมจัดเก็บข้อมูลในรูปแบบ:
Data Warehouse →โกดังข้อมูลเชิงโครงสร้าง (Structured Data) จากหลายแหล่ง
Data Mart →คลังข้อมูลย่อย แบ่งตามภูมิภาค/หน้าทีÉ (เช่น Sales Mart, Marketing Mart)
Data Lake →แหล่งรวมข้อมูลทุกประเภท (Structured, Semi-structured, Unstructured)
3.ตรวจสอบคุณภาพการดึงและบันทึก (Verify and Document Extraction)
•ต้องตรวจสอบคุณภาพของข้อมูลทีÉดึงมา และบันทึกขัÊนตอนเพืÉอความโปร่งใสประเภทของข้อมูล
•Structured Data: ข้อมูลทีÉมีโครงสร้างแน่นอน เช่น General 
Ledger, Relational Database, Spreadsheet
•Semi-structured Data: ข้อมูลทีÉมีรูปแบบบางส่วน เช่น CSV, XML, 
JSON, Log Files
•Unstructured Data: ข้อมูลไร้โครงสร้าง เช่น รูปภาพ, วิดีโอ, เสียง, 
โซเชียลมีเดีย, เอกสาร
ข้อควรระวัง
Data Warehouse →ใหญ่และซับซ้อนมาก (เช่น Facebook ปี 2014 
มี 300 Petabytes ใน 800,000 ตาราง)
Data Mart →เล็กกว่า เข้าถึงเร็วกว่า และควบคุมสิทธิÍผู้ใช้ได้ดีกว่า
Data Lake →ยืดหยุ่นสูง แต่ถ้าไม่มีการจัดการ อาจกลายเป็น Dark 
Data (ข้อมูลทีÉมีแต่ไม่ได้ใช้) หรือ Data Swamp ( ข้อมูลไม่ถูกจัดทําเอกสาร 
ทําให้ใช้ไม่ได้)

18
ข้อควรระวัง (Cautions) จุดแข็ง (Strengths) ประเภทข้อมูล (Data Types) โครงสร้าง (Structure) ประเภท (Type)
- ขนาดใหญ่มาก → เข้าถึงช้า 
- ค่าใช้จ่ายสูงในการจัดเก็บและ
บํารุงรักษา- รวมข้อมูลจากหลายแหล่งใน
องค์กร - รองรับการวิเคราะห์เชิง
ลึก - มาตรฐานสูงและน่าเชืÉอถือข้อมูลเชิงโครงสร้าง (Structured 
Data) เช่น General Ledger, 
Relational Database, Spreadsheetโครงสร้างชัดเจน (Highly 
Structured) เหมาะสําหรับ 
Reporting / BIData Warehouse
- มักไม่ครอบคลุมข้อมูลทัÊง
องค์กร - อาจเกิดความซํÊาซ้อน
ของข้อมูลระหว่าง Mart ต่าง ๆ- ขนาดเล็ก → เข้าถึงเร็ว - 
ควบคุมสิทธิÍการเข้าถึงง่าย - ตอบ
โจทย์เฉพาะหน่วยงานข้อมูลเชิงโครงสร้าง (Structured 
Data) เฉพาะด้าน เช่น Sales Mart, 
Marketing Martโครงสร้างชัดเจน แต่ขอบเขตเล็ก
กว่า (Smaller, Structured)Data Mart
- เสีÉยงกลายเป็น Dark Data 
(เก็บแต่ไม่ใช้) - เสีÉยงกลายเป็น 
Data Swamp ( ข้อมูลไม่ถูก
จัดทําเอกสาร ใช้ต่อไม่ได้)- รองรับข้อมูลทุกประเภท - รวม
ข้อมูลภายในและภายนอกองค์กร 
- รองรับการประมวลผลแบบ 
Big Data & AIรวมได้ทุกประเภท: - Structured 
(โครงสร้างชัดเจน) - Semi-structured 
(CSV, XML, JSON) -Unstructured 
(ภาพ, วิดีโอ, เสียง, Social Media)ยืดหยุ่นสูง (Flexible, Scalable)  
เพิÉมพลัง AI/ML, Advanced 
AnalyticsData Lakeตารางเปรียบเทียบ Data Warehouse, Data Mart และ Data LakeEXTRACT, TRANSFORM, AND LOAD RELEVANT DATA

19การออกแบบ Data Warehouse, Data Mart และ Data Lake
การออกแบบ Data Warehouse, Data Mart และ Data Lake ได้หลายวิธี เช่น:
1.Data Warehouse →Data Mart
•ออกแบบ Data Warehouse ให้เชืÉอมต่อกับแหล่งข้อมูลธุรกรรม (Transaction 
Data) หรือข้อมูลเชิงโครงสร้าง (Structured Data) ทัÊงหมด
•จากนัÊนสร้าง Data Mart โดยอ้างอิงจากข้อมูลใน Data Warehouse
2.Data Mart →Data Warehouse
•ออกแบบ Data Mart ให้เชืÉอมต่อโดยตรงกับแหล่งข้อมูลธุรกรรม
•นําข้อมูลจาก Data Mart ต่าง ๆ มารวม ( Aggregate) เป็น Data Warehouse
3.Data Warehouse →Data Lake(ทัÉวไปทีÉสุด)
•โดยทัÉวไป Data Warehouse ถือเป็นฐานหลัก และมักถูกใช้ในการสร้าง Data 
Lake
4.Independent Access
•องค์กรอาจออกแบบให้ Data Warehouse, Data Mart และ Data Lake 
เข้าถึงแหล่งข้อมูลโดยตรงอย่างอิสระ
การเข้าใจโครงสร้างการออกแบบเหล่านีÊเป็นสิÉงสําคัญ เพราะจะช่วยผู้ใช้งานระบุได้ว่าข้อมูลทีÉ
ต้องการเก็บไว้ทีÉใด และสามารถเข้าถึงได้อย่างถูกต้องความสําคัญของ Data Dictionary และ Metadata
เพืÉอทําความเข้าใจโครงสร้างข้อมูล วิธีทีÉดีทีÉสุดคือการศึกษา Data 
DictionaryซึÉงเก็บ Metadata(ข้อมูลทีÉใช้อธิบายข้อมูล) เช่น:
จํานวนตัวอักษรสูงสุดในฟิลด์ (Field Length)
ประเภทข้อมูล (Data Type) เช่น Integer, Text, Date/Time
รูปแบบข้อมูล (Data Format)
ประโยชน์ของ Metadata ทีÉถูกต้องและทันสมัย
ช่วยป้องกันไม่ให้ข้อมูลกลายเป็น Dark Data(ข้อมูลทีÉถูกเก็บแต่
ไม่ได้วิเคราะห์)
ลดความเสีÉยงทีÉ Data Lake จะกลายเป็น Data Swamp (ข้อมูลทีÉ
ไร้การจัดทําเอกสาร ใช้งานต่อไม่ได้)
ดังนัÊน การตรวจสอบ Data Dictionary อย่างรอบคอบ เพื Éอเข้าใจทัÊง 
ข้อมูล (Data)และ โครงสร้างพืÊนฐานของข้อมูล (Underlying 
Objects)จึงเป็นขัÊนตอนสําคัญของกระบวนการ ETLEXTRACT, TRANSFORM, AND LOAD RELEVANT DATA

20EXTRACT, TRANSFORM, AND LOAD RELEVANT DATA
แผนภาพแสดงโครงสร้างการเชืÉอมโยง Data Warehouse –Data Mart –Data Lake
แบบทีÉ 1 แบบทีÉ 2 แบบทีÉ 3
1. Retail (ค้าปลีก)
เดิม:
•Data Warehouse เก็บข้อมูลยอดขาย POS, Inventory, 
Finance
•ใช้ทํารายงาน KPI เช่น ยอดขายรายวัน, Margin, Stock 
level
เปลีÉยนเป็น Data Lake:
•รวมข้อมูลจาก POS + clickstream บน e-commerce 
+ social media feedback + IoT sensor จากห้าง
•นําไปทํา customer 360 view และ personalized 
recommendation (AI/ML)3. Manufacturing ( โรงงาน/อุตสาหกรรม)
เดิม:
•DWH เก็บข้อมูลการผลิต, ต้นทุน, BOM 
→ใช้ทํา cost analysis
เปลีÉยนเป็น Data Lake:
•เก็บ IoT sensor data จากเครืÉองจักร, 
maintenance logs, weather data
•ใช้ทํา predictive maintenance, yield 
optimization2. Banking (ธนาคาร)
เดิม:
DWH เก็บ transaction, loan portfolio, balance sheet 
→ใช้ทํารายงานตามกฎหมาย (regulatory reporting)
เปลีÉยนเป็น Data Lake:
•เก็บข้อมูลธุรกรรมแบบ real-time, call center logs, 
mobile app clickstream
•ใช้ทํา fraud detection, customer churn prediction, 
credit scoring modelจาก Enterprise Data Warehouse →Sales Data 
Mart
•DWH เก็บข้อมูลทัÊงหมด: ลูกค้า, สินค้า, inventory, 
accounting, HR
•Sales Data Mart →ดึงเฉพาะข้อมูล ยอดขาย, 
ลูกค้า, campaign
•ใช้ตอบคําถาม: “Top 10 ลูกค้าปีนีÊ?”, “ยอดขายตาม 
region เดือนนีÊเทียบกับปีก่อน?”

ตาราง: ตัวอย่าง METADATA ใน DATA DICTIONARY
21
Example (ตัวอย่างค่า) Format (รูปแบบข้อมูล)Allowed Characters 
(อักขระทีÉอนุญาต)Data Type (ชนิดข้อมูล) Field Name (ชืÉอฟิลด์)
00012345 8 หลัก (Fixed Length) ตัวเลข 0–9 เท่านัÊน Integer (จํานวนเต็ม) Customer_ID
Somchai Prasert สูงสุด 100 ตัวอักษรตัวอักษร A–Z, a–z, เว้น
วรรคText (ข้อความ) Customer_Name
2025-09-02 YYYY-MM-DD ตัวเลขและตัวแบ่งวันทีÉ Date/Time (วันทีÉ/เวลา) Invoice_Date
12500.75 สูงสุด 2 ตําแหน่งทศนิยม ตัวเลข 0–9 และ “.” Decimal (ทศนิยม) Invoice_Amount
somchai@email.com สูงสุด 150 ตัวอักษรตัวอักษร A–Z, a–z, 
ตัวเลข, @, .Text (ข้อความ) Email_Address
PRD202500110 ตัวอักษร (Fixed 
Length)ตัวอักษร A–Z และตัวเลข 
0–9Text/Alphanumeric 
(อักษรผสมตัวเลข)Product_Code

EXTRACT, 
TRANSFORM, 
AND LOAD 
RELEVANT 
DATA
22
เครืÉองมือ/แนวทาง (Tools / Guidelines) สิÉงทีÉต้องทํา (What to Do) ขัÊนตอน (Step)
เชืÉอมโยงกับ Analytics Mindset –Ask the 
Right Questionระบุว่าข้อมูลใดจําเป็นต่อการตอบคําถาม/
วิเคราะห์1. กําหนดความต้องการข้อมูล (Define Data
Needs)
Internal Control / Approval WorkflowตรวจสอบสิทธิÍและขออนุญาตจาก Data 
Owner2. ขออนุญาตการเข้าถึง (Access 
Permission)
Tools: SQL, Data Export Function, ETL 
Toolเลือกรูปแบบไฟล์: - Separate Files -Flat 
File (แนะนํา)3. ดึงข้อมูล (Perform Extraction)
” หรือ “,” และกํากับด้วย Text Qualifier เช่น 
“ ”ใช้ตัวแบ่งฟิลด์ (Delimiter) เช่น “ 4. กําหนด Delimiter และ Text Qualifier
Relational Database Keyรวม Primary Key เพืÉอให้ระบุข้อมูลไม่ซํÊา 
และรวมข้อมูลรอบใหม่ได้5. ระบุ Primary Key
Batch Processing Controls / Record 
Countตรวจสอบความครบถ้วน/ถูกต้อง เช่น 
เปรียบเทียบจํานวน Record กับ Source6. ตรวจสอบคุณภาพการดึง (Verify Quality)
Data Extraction Log / ETL 
DocumentationบันทึกขัÊนตอนและผลการดึงข้อมูลเพืÉอ Audit 
Trail7. จัดทําบันทึก (Document Extraction)ตารางสรุปขัÊนตอนการดึงข้อมูล (Extract Data)

23
EXTRACT, 
TRANSFORM, 
AND LOAD 
RELEVANT 
DATA

ตาราง BEST PRACTICES หลังการดึงข้อมูล (POST-EXTRACTION BEST PRACTICES)
24
ประโยชน์ (Benefits) สิÉงทีÉต้องทํา (What to Do) ขัÊนตอน (Step)
- ยืนยันความถูกต้องของกระบวนการดึง
ข้อมูล - สร้างหลักฐานการตรวจสอบ 
(Audit Evidence)เลือก Sample Records แล้วทําการดึง
ข้อมูลซํÊา จากนัÊนเปรียบเทียบกับ Full 
Data Extract1. Reperform Data Extraction
- ผู้ใช้เข้าใจข้อมูลได้ตรงกัน - รองรับการ
ใช้งานต่อเนืÉองสร้าง Data Dictionary ใหม่จากผลการ
ดึงข้อมูล - คัดลอกจาก Source Data 
Dictionary -อัปเดตฟิลด์ทีÉมีการ
เปลีÉยนแปลง2. Create New Data Dictionary
- ป้องกันการเกิด Data Swamp -เพิÉม
ความโปร่งใสในการใช้งานข้อมูลระบุแหล่งทีÉมาของข้อมูล (Source) ใน 
Metadata และเพิÉมคําอธิบายสําหรับ
ข้อมูลทีÉนําเข้าจากภายนอก3. Add Metadata
- ป้องกันความเข้าใจผิดในอนาคต - ช่วย
ให้นักวิเคราะห์ใช้ข้อมูลได้อย่างมี
ประสิทธิภาพหากมีข้อมูลทีÉไม่เคยถูกกําหนดไว้ใน 
Data Dictionary เดิม ต้องกําหนด 
Field ใหม่อย่างถูกต้อง4. Define New Fields Clearly

25EXTRACT, 
TRANSFORM,
AND LOAD 
RELEVANT 
DATAการแปลงข้อมูล (Data Transformation Process) คือการทําให้ข้อมูลมีมาตรฐาน (Standardizing) มีโครงสร้างทีÉชัดเจน 
(Structuring) และการทําความสะอาดข้อมูล (Cleaning) เพืÉอให้ข้อมูลอยู่ในรูปแบบทีÉพร้อมสําหรับการวิเคราะห์ กระบวนการนีÊใช้เวลา
และมีความซับซ้อนสูง ภาพรวมของกระบวนการ 4 ขัÊนตอน คือ:
ความเสีÉยงถ้าไม่ทํา (Risks if Skipped) สิÉงทีÉต้องทํา (What to Do) ขัÊนตอน (Step)
- เลือกข้อมูลผิดหรือไม่ครบถ้วน - ข้อมูลไม่
ตอบโจทย์วิเคราะห์ - ต้องทํา ETL ซํÊา 
เสียเวลาและทรัพยากร- ศึกษาข้อมูลทีÉ Extract มา - สร้าง/ตรวจสอบ Data 
Dictionary -กําหนดข้อกําหนด เช่น Format, 
Delimiter, Filtering, Level of Detail1. เข้าใจข้อมูลและผลลัพธ์ทีÉ
ต้องการ (Understand the 
Data & Desired Outcome)
- ข้อมูลไม่สามารถรวมกันได้ - วิเคราะห์
ผิดพลาดเพราะโครงสร้างไม่ตรงกัน - ใช้เวลา
มากในการแก้ไขภายหลัง- รวมข้อมูลจากหลายระบบให้อยู่ใน Format เดียวกัน - 
ลบ/แก้ไขข้อมูลซํÊา ข้อมูลผิดพลาด - สร้างความ
สมํÉาเสมอ (Consistency)2. ทําให้ข้อมูลมีมาตรฐาน จัด
โครงสร้าง และทําความสะอาด 
(Standardize, Structure & 
Clean)
- ข้อมูลแม้ถูกต้อง แต่ไม่ตรงกับความ
ต้องการวิเคราะห์ → ใช้ไม่ได้ - เกิด
ข้อผิดพลาดในการตัดสินใจ- ตรวจสอบความถูกต้อง (Accuracy) และความ
ครบถ้วน (Completeness) - ตรวจสอบความ
สอดคล้องกับวัตถุประสงค์ (Data Requirements)3. ตรวจสอบคุณภาพและความ
สอดคล้อง (Validate Data 
Quality & Requirements)
- ผู้ใช้งานในอนาคตไม่เข้าใจข้อมูล - ใช้
ข้อมูลผิดบริบท (Misuse) -เสีÉยงกลายเป็น 
Data Swamp- อัปเดต Data Dictionary - บันทึกการเปลีÉยนแปลง
และเหตุผล - จัดทําเอกสารสําหรับการใช้งานต่อไป4. จัดทําเอกสาร (Document 
the Process)

26EXTRACT, 
TRANSFORM, 
ANDLOAD 
RELEVANT 
DATAการโหลดข้อมูล (Loading Data)
เมืÉอข้อมูลถูกจัดโครงสร้าง (Structured) และทําความสะอาด (Cleaned) แล้ว ก็พร้อมทีÉจะถูกนําเข้า (Imported) ไปยังเครืÉองมือหรือโปรแกรมที É
จะใช้ในการวิเคราะห์ หากข้อมูลได้รับการแปลง (Transformed) อย่างถูกต้องแล้ว ขัÊนตอนนีÊมักจะดําเนินการได้อย่างรวดเร็วและไม่ซับซ้อน 
อย่างไรก็ตาม มีข้อควรพิจารณาทีÉสําคัญดังนีÊ:
ความเสีÉยงถ้าไม่ทํา (Risks if Skipped) สิÉงทีÉต้องทํา (What to Do) ขัÊนตอน (Step)
- โปรแกรมไม่สามารถนําเข้าข้อมูลได้ - ละเมิดกฎ 
Referential Integrity →เกิดข้อมูลผิดพลาด- บันทึกไฟล์ให้อยู่ในรูปแบบทีÉระบบรองรับ 
เช่น Text File + Delimiters, XBRL, 
JSON, Relational Tables - โหลดข้อมูล
ตามลําดับทีÉสอดคล้องกับ Referential 
Integrity1. จัดเก็บข้อมูลในรูปแบบ/โครงสร้างทีÉถูกต้อง 
(Correct Format & Structure)
- โปรแกรมตีความข้อมูลผิดพลาด - เกิดค่าผิด เช่น ติด
ลบไม่ถูกต้อง หรือวันทีÉสลับเดือน/วัน- ลบคอมม่า (,) ทีÉคัÉนหลักพัน - ใช้
เครืÉองหมายลบ (–) แทนวงเล็บสําหรับค่าติด
ลบ - ใช้ Date Format มาตรฐานเดียวกัน2. ทําให้รูปแบบข้อมูลเป็นมาตรฐาน 
(Standardize Data Formats)
- ผู้ใช้งานภายหลังไม่เข้าใจข้อมูล - ใช้ข้อมูลผิดบริบท - 
อาจต้องทํา ETL ซํÊาทัÊงหมด- จัดทํา Data Dictionary ใหม่ทีÉอธิบายทุก
ฟิลด์ - บันทึก Metadata เช่น แหล่งทีÉมาของ
ข้อมูล และการเปลีÉยนแปลงระหว่าง
กระบวนการ3. อัปเดต/สร้าง Data Dictionary (Update 
or Create Data Dictionary)

3. APPLY APPROPRIATE DATA ANALYTIC TECHNIQUES
27ตารางทีÉ 1: 4 ประเภทของ Data Analytics
ตัวอย่าง (Example) จุดประสงค์ (Purpose) คําถามหลัก (Key Question) ประเภท (Type)
ROI, Gross Marginทําความเข้าใจอดีตหรือสถานการณ์
ปัจจุบันWhat happened? / What is 
happening?Descriptive Analytics
IT Budget ↑→Efficiency ↑ค้นหาความสัมพันธ์เชิงเหตุผล (Causal 
Relationships)Why did this happen? Diagnostic Analytics
ราคาหุ้น, อัตราแลกเปลีÉยน พยากรณ์สิÉงทีÉจะเกิดขึÊนในอนาคต What might happen? Predictive Analytics
Loan Approval Algorithmเสนอแนวทางการตัดสินใจหรือการลงมือ
ทําWhat should be done? Prescriptive Analytics

3. APPLY APPROPRIATE DATA ANALYTIC TECHNIQUES
28ตารางทีÉ 2: ระดับทักษะ
ตัวอย่างการปฏิบัติ (Example in Practice) ความหมาย (Meaning) ระดับ (Level)
รู้ว่า Python ใช้ทํา Data Analysis ได้ แต่ยังเขียนโค้ดเอง
ไม่ได้รู้จักว่าเครืÉองมือ/เทคนิคคืออะไรและทํา
อะไรได้ แต่ทําเองไม่ได้Awareness (การรับรู้)
เขียน SQL Query ง่าย ๆ ได้ แต่ต้องเปิดคู่มือเมืÉอเจอโจทย์
ซับซ้อนเคยทํางานคล้ายกันมาก่อน สามารถทําได้
แต่ต้องทบทวนหรือหาคู่มือช่วยWorking Knowledge ( ความรู้เชิงปฏิบัติ)
ได้รับงาน Data Analytics แล้วสามารถใช้ Python, SQL, 
Machine Learning ทํางานได้ครบถ้วนทําได้ทันที เข้าใจลึก สามารถรับผิดชอบ
โครงการเต็มรูปแบบMastery (ความเชีÉยวชาญ)

4. INTERPRET AND SHARE THE RESULTS WITH STAKEHOLDERS
29การตีความผลลัพธ์ (Interpreting Results) และ การสืÉอสารผลลัพธ์ (Sharing Results / Data Storytelling)  ตารางนีÊช่วยให้เห็นว่า 
Interpreting Results = Internal Understanding ในขณะทีÉ Sharing Results = External Communication
การสืÉอสารผลลัพธ์ (Sharing Results / Data Storytelling)การตีความผลลัพธ์ (Interpreting 
Results)ด้าน (Aspect)
ถ่ายทอดผลวิเคราะห์ให้ Stakeholders เข้าใจและตัดสินใจได้ เข้าใจความหมายของผลวิเคราะห์อย่างถูกต้อง จุดมุ่งหมาย (Objective)
- ความแตกต่างของผู้ฟัง (ประสบการณ์, ความรู้ด้าน Data) -เลือก
รูปแบบการเล่าเรื Éอง/Visualization ทีÉเหมาะสม- ความสับสนระหว่าง Correlation vs 
Causation และConfirmation Bias ( ตีความ
ตามความเชืÉอ/ความปรารถนา)ความท้าทาย 
(Challenges)
-CIO ต้องการเห็นแนวโน้มค่าใช้จ่ายด้านเทคโนโลยี → ใช้ Line 
Chart บน Dashboard แทนการรายงานเป็นตัวเลข- ยอดขายอุปกรณ์หิมะสูงขึÊนเพราะโฆษณา? หรือ
เพราะเข้าหน้าหนาว? - ผู้จัดการให้คะแนน
พนักงานเกินจริงเพราะเป็นเพืÉอนตัวอย่าง (Example)
- เริÉมจากคําถามต้นทาง (Right Question) - เข้าใจผู้ฟัง (Audience) 
-ใช้ Visualization ทีÉเหมาะสม (Dashboard, Graphs) - ทําให้ง่าย 
เน้นประเด็น และสืÉอสารอย่างมีจริยธรรม- ฝึกการตีความอย่างเป็นกลาง (Objective) -ใช้
ความรู้และการฝึกฝนเพืÉอเข้าใจผลลัพธ์อย่าง
แท้จริงแนวทางทีÉดี (Best 
Practices)

4. INTERPRET AND SHARE THE RESULTS WITH STAKEHOLDERS
30ตารางสรุป Interpreting vs Sharing vs Visualization Principles
แนวทางทีÉดี (Best Practices) ความท้าทาย/ความเสีÉยง (Challenges/Risks) จุดมุ่งหมาย (Objective) หมวด (Category)
- ตีความอย่าง เป็นกลาง (Objective)-ฝึกฝน
เพืÉอเพิÉมความแม่นยํา- สับสนระหว่าง Correlation vs Causation -
Confirmation Bias : ตีความเข้าข้างความเชืÉอเดิมเข้าใจความหมายทีÉแท้จริง
ของผลวิเคราะห์Interpreting Results (การ
ตีความผลลัพธ์)
- เริÉมจาก คําถามต้นทาง (Right Question) -
วิเคราะห์ ผู้ฟัง (Audience)-ใช้ Visualization
ทีÉเหมาะสม- ระดับความรู้และประสบการณ์ของผู้ฟังต่างกัน - เลือก
รูปแบบการเล่าเรื Éองไม่ตรงกลุ่มเป้าหมายถ่ายทอดผลวิเคราะห์ให้ 
Stakeholders เข้าใจง่าย
และใช้ตัดสินใจได้Sharing Results / Data 
Storytelling(การสืÉอสาร
ผลลัพธ์)
- เลือกชนิด Visualization ให้เหมาะ - ทําให้
เรียบง่าย (Simplify) -เน้นสิÉงสําคัญ 
(Emphasize) - แสดงข้อมูลอย่าง มีจริยธรรม 
(Ethical)- นําเสนอซับซ้อนเกินไป - ขาดจริยธรรมในการแสดง
ข้อมูลใช้กราฟ/แผนภาพช่วย
สืÉอสารข้อมูลให้ชัดเจนและ
ทรงพลังVisualization Principles
(หลักการ Visualization)

ADDITIONAL DATA ANALYTICS CONSIDERATIONS
•การทํางานอัตโนมัติ (Automation)
•ความหมาย:
Automation คือการใช้เครืÉองจักรหรือซอฟต์แวร์ให้ทํางานโดยอัตโนมัติแทนทีÉมนุษย์ เช่น เดิมต้องคัดลอกและวางข้อมูลด้วยมือ 
→ เขียนโปรแกรมให้ทําแทนได้
31ความเสีÉยง (Risks) งานทีÉเหมาะสม (Best Fit Tasks) ตัวอย่าง (Examples) ประเภท (Type)
- หาก Logic ผิด → ทําผิดซํÊารวดเร็ว - ไม่อัปเดต Bot 
เมืÉอ Process เปลีÉยน → ข้อมูลผิดพลาด- งานซํÊาซาก (Repetitive) -ทําบ่อย 
(Frequent) -ใช้เวลานาน (Time-
consuming) - มีกฎเกณฑ์ชัดเจน (Rules-
based)-Copy/Paste Data -Consolidate 
Tax Data ด้วย Bot -Balance 
Journal EntriesBasic Automation (RPA 
–Robotic Process 
Automation)
- ความแม่นยําขึÊนอยู่กับ Training Data -Bias ของ 
Algorithm -ต้องการ Monitoring และการปรับแต่งต่อเนืÉอง- งานทีÉต้อง เรียนรู้/คาดการณ์ - วิเคราะห์
ข้อมูลซับซ้อน - ตัดสินใจเชิงกลยุทธ์-Tableau “Ask Data” -Aera 
Technology (Self-driving 
enterprise) -Forecast Demand 
& Auto-replenish InventoryAdvanced Automation
(AI, Machine Learning, 
Cognitive Computing)

ADDITIONAL DATA ANALYTICS CONSIDERATIONS
32
กระบวนการ Automation  ส่งผลต่อมนุษย์ คือ (Human Element of Automation)
•พนักงานมักกังวลว่าการนํา Automation มาใช้จะทําให้ สูญเสียงาน
•จริงอยู่ Automation อาจถูกใช้เพืÉอลดจํานวนพนักงาน แต่ก็สามารถใช้เพืÉอลด งานซํÊาซากและน่าเบืÉอ เพืÉอให้
พนักงานมีเวลาไปทํางานทีÉ สร้างคุณค่า (Value-Added Tasks) มากกว่า
•องค์กรควร พิจารณาผลกระทบต่อบุคลากร และเตรียมการบริหารจัดการความกังวลเหล่านีÊ ก่อนเริÉมโครงการ 
Automation

ADDITIONAL DATA ANALYTICS CONSIDERATIONS
33Data Analytics ไม่ใช่เครืÉองมือทีÉเหมาะสมเสมอไป (Data Analytics Is Not Always the Right Tool)
•ข้อมูลทีÉน่าเชืÉอถืออาจ ไม่มี สําหรับคําถามบางประเภท → ทําให้ Data Analytics ไม่สามารถให้
คําตอบทีÉถูกต้องได้
•Human Judgment & Intuition อาจมีความสําคัญ เช่น การประเมินปัจจัยด้านอารมณ์ ความรู้สึก 
หรือบริบททีÉไม่สามารถวัดผลเชิงตัวเลขได้
•ตัวอย่าง: หาก Data Analysis แสดงว่าการโกง ( Fraud) จะทําให้ได้ผลตอบแทนสูงและมีโอกาสถูกจับ
น้อย → CEO ทีÉมีจริยธรรมจะรู้ว่าการกระทําดังกล่าว ผิดแม้ผลวิเคราะห์บอกว่า “คุ้ม”

TRANSFORMING 
DATA

INTRODUCTION
35ปัญหาคุณภาพข้อมูล (Data Quality Problem)
•IBM ประมาณการว่า ข้อมูลคุณภาพตํÉา สร้างความสูญเสียต่อเศรษฐกิจ
สหรัฐอเมริกา 3.1 ล้านล้านดอลลาร์/ปี
•1 ใน 3 ของผู้ตัดสินใจทางธุรกิจ ไม่เชืÉอถือข้อมูล ทีÉใช้ตัดสินใจ
•ผลสํารวจโดย Experian: ผู้บริหารเชืÉอว่า 26% ของข้อมูลทัÊงหมดไม่
ถูกต้อง และปัญหากําลังทวีความรุนแรงขึÊน
สาเหตุของข้อมูลคุณภาพตํÉา (Dirty Data Causes)
•ข้อผิดพลาดเชิงเทคนิค: ข้อมูลไม่สมบูรณ์ (Incomplete), ล้าสมัย (Outdated), ซํÊาซ้อน 
(Duplicate), หรือผิดพลาดเล็กน้อย (Typos, Spelling Mistakes)
•ปัจจัยภายนอก: การเปลีÉยนแปลงในโลกจริง (เช่น ลูกค้าเปลีÉยนเบอร์โทรศัพท์)
•การรวมข้อมูล: จากหลายแหล่งทีÉใช้รูปแบบต่างกัน
•ข้อผิดพลาดในการจัดการ: Human Errors ในการบันทึกหรือประมวลผลข้อมูล
แนวทางแก้ไข (Solutions)
•อุดมคติ: เก็บและจัดเก็บข้อมูลทีÉสะอาดตัÊงแต่แรก (Capture & Store Clean Data)
•การควบคุมภายใน:  (ตามบททีÉ 10–13) เพืÉอสนับสนุนการเก็บข้อมูลทีÉถูกต้อง
•ความเป็นจริง: แม้มี Internal Controls ข้อมูลก็ยังอาจกลายเป็น Dirty Data →
จึงต้องใช้กระบวนการ Data Transformation
กระบวนการฟืÊนฟูคุณภาพข้อมูล (Four-Step Data Transformation Process)
1.Structure the Data –จัดโครงสร้างข้อมูลให้เป็นระบบ
2.Standardize the Data –ทําให้ข้อมูลอยู่ในรูปแบบมาตรฐานเดียวกัน
3.Clean the Data –แก้ไขหรือลบข้อมูลทีÉผิดพลาด ซํÊาซ้อน หรือไม่สมบูรณ์
4.Validate the Data –ตรวจสอบคุณภาพ ความถูกต้อง และความสมบูรณ์ของข้อมูล

ATTRIBUTES OF HIGH-QUALITY DATA
36
ขัÊนตอน Transformation ทีÉเกีÉยวข้อง ตัวอย่าง (Example) ความหมาย (Meaning) คุณลักษณะ (Attribute)
Clean, Validate เบอร์โทรลูกค้าปัจจุบันตรงกับความจริง ข้อมูลสะท้อนความจริงได้ถูกต้อง Accuracy (ความถูกต้อง)
Structure, Clean รายการคําสัÉงซืÊอบันทึกทัÊงชืÉอ, ทีÉอยู่, วันทีÉ ไม่มีข้อมูลทีÉขาดหาย Completeness ( ความสมบูรณ์)
Standardize“Customer ID” เหมือนกันทัÊง ERP และ 
CRMข้อมูลเหมือนกันในทุกระบบ/ทุกทีÉ Consistency ( ความสอดคล้องกัน)
Validate ยอดขายรายวันอัปเดตภายในวันถัดไป ข้อมูลเป็นปัจจุบันและพร้อมใช้งาน Timeliness (ความทันเวลา)
Clean ลูกค้า 1 คน = 1 Record เท่านัÊน ไม่มีข้อมูลซํÊาทีÉไม่จําเป็น Uniqueness (ความไม่ซํÊาซ้อน)
Structure, Validateใช้ข้อมูล Vendor ไม่ใช่ Customer สําหรับ
วิเคราะห์ Supplier Performanceข้อมูลสอดคล้องกับวัตถุประสงค์ทีÉใช้ Relevance (ความเกีÉยวข้อง)คุณลักษณะของข้อมูลคุณภาพสูง (High-Quality Data Attributes)
ในทุก ๆ การทํางานกับข้อมูล สิÉงสําคัญคือ ตรวจสอบให้แน่ใจว่าข้อมูลมีคุณลักษณะของข้อมูลคุณภาพสูงดังนีÊ

DATA STRUCTURING

DATA STRUCTURING
38ความหมาย (Definition)Data Structure = วิธีการจัดเก็บข้อมูล รวมถึง ความสัมพันธ์ระหว่างฟิลด์ข้อมูลต่าง ๆData Structuring = 
กระบวนการปรับเปลีÉยนการจัดระเบียบและความสัมพันธ์ของข้อมูล เพืÉอเตรียมพร้อมสําหรับการวิเคราะห์
ความสําคัญ (Importance)
ข้อมูลทีÉถูก Extracted ออกมา มักจะยัง ไม่เหมาะสม
สําหรับการวิเคราะห์ทันที
ต้องมีการจัดโครงสร้าง (Structuring) เพืÉอให้ข้อมูลสามารถ
นําไปใช้วิเคราะห์ได้อย่างมีประสิทธิภาพเทคนิคทีÉใช้ในการ Structuring (Techniques)
1. Aggregating Data: รวมข้อมูลในระดับรายละเอียดทีÉแตกต่างกัน 
(Summarization)
2. Joining Data: เชืÉอมโยงข้อมูลจากหลายแหล่ง/หลายตารางเข้าด้วยกัน 
(Integration)
3. Pivoting Data: หมุนตารางข้อมูลเพืÉอเปลีÉยนมุมมองหรือนําเสนอข้อมูลใน
รูปแบบทีÉเหมาะกับการวิเคราะห์
Key Insight:
การจัดโครงสร้างข้อมูล (Data Structuring) เป็นขัÊนตอนสําคัญระหว่าง การ Extract ข้อมูล และ การวิเคราะห์ข้อมูล 
เพราะเป็นตัวเชืÉอมทีÉทําให้ข้อมูลพร้อมใช้และเชืÉอมโยงกันได้อย่างถูกต้อง

1. AGGREGATE DATA
39ความหมายของ Aggregate Data
Aggregate Data ( ข้อมูลทีÉสรุปรวม) = การนําเสนอข้อมูลในรูปแบบสรุป (Summarized Form)  มี รายละเอียดน้อยกว่า (Fewer Details) เมืÉอเทียบกับ 
Disaggregated Data ( ข้อมูลเชิงละเอียด/ดิบ)
ตัวอย่ง:การวิเคราะห์ยอดซืÊอจาก Vendor →สามารถสรุป
รวมเป็น Total Units Purchased และ Total Expenditures
ต่อ Vendor  แต่หากสรุปรวมแล้ว จะไม่สามารถรู้ได้ว่า ซืÊอสินค้า
ใดบ้าง ต้องย้อนกลับไปยัง Disaggregated Data เท่านัÊน

1. AGGREGATE DATA
40
แนวปฏิบัติทีÉดี (Best Practice) ข้อจํากัด (Limitations) การใช้งาน (Use) ตัวอย่าง (Example) คําจํากัดความ (Definition) ประเภทข้อมูล (Type)
เก็บข้อมูลในรูปแบบนีÊเป็นหลักปริมาณข้อมูลมาก → การ
ประมวลผลอาจช้าใช้สําหรับ การวิเคราะห์เชิงลึก, 
Drill-down, Audit, 
Traceabilityรายการขายแต่ละครั Êง, ใบสัÉงซืÊอ, 
รายการสินค้าข้อมูลทีÉบันทึกในระดับ เหตุการณ์
เดีÉยว/ธุรกรรม โดยไม่มีการสรุป
รวมDisaggregated Data (ข้อมูลเชิง
ละเอียด)
สร้าง Aggregation ผ่าน 
Query/Report ตามความ
ต้องการ แทนทีÉจะเก็บแบบสรุป
รวมถาวรรายละเอียดหายไป ไม่สามารถ
ย้อนวิเคราะห์ถึงธุรกรรมย่อยได้ใช้สําหรับ รายงานสรุป, การ
ตัดสินใจเชิงกลยุทธ์, 
Performance Evaluationยอดขายรวมต่อ Sales 
Manager, ยอดขายรวมทัÊงหมด
ในงบกําไรขาดทุนข้อมูลทีÉถูก สรุปรวม/รวมยอด  
ตามมิติทีÉสนใจAggregated Data (ข้อมูลสรุป
รวม)
Key Takeaway:
Disaggregated = เก็บ (เก็บข้อมูลดิบเพืÉอความยืดหยุ่น)
Aggregated = ใช้ (ใช้สร้างรายงาน/การวิเคราะห์ แต่ไม่ใช่รูปแบบเก็บถาวร)ความสําคัญในกระบวนการ Transforming Data
ต้องเข้าใจ ระดับการสรุปรวม (Level of Aggregation) ของแต่ละแหล่งข้อมูล
การรวมข้อมูลทีÉสรุปในระดับต่างกัน อาจก่อให้เกิด ปัญหาในการวิเคราะห์ตารางเปรียบเทียบ Aggregate Data vs Disaggregate Data

2. DATA JOINING
41การ Join ข้อมูลในฐานข้อมูล (Database Joins in ETL Process)
1. ความสําคัญของ Join
การ Query ฐานข้อมูลมักต้อง รวมข้อมูลจากหลายตาราง (Join)เพืÉอสร้างตาราง
เดียวทีÉพร้อมสําหรับการวิเคราะห์
เป็นขัÊนตอนสําคัญใน ETL ProcessโดยเฉพาะเมืÉอข้อมูลสุดท้ายถูก Export 
ออกมาเป็น Flat File
2. ตัวอย่าง (S&S Dataset Example)
ตารางทีÉได้จากการ Join มีข้อมูลมาจากหลายแหล่ง เช่น:
Vendor Table→ฟิลด์ทีÉเกีÉยวกับผู้ขาย เช่น Vendor Name, Discount
Product Table→ฟิลด์ทีÉเกีÉยวกับสินค้า เช่น ProdCat, ProdID
Transaction Table →ฟิลด์ทีÉเกีÉยวกับธุรกรรมการซืÊอ เช่น UnitsPurch, 
TotalCosts3. ผลลัพธ์
หลังจาก Join →ได้ Flat File Table ทีÉรวมฟิลด์จากทัÊง Vendor + Product + Transaction
เข้าด้วยกันตารางเดียวนีÊทําให้นักวิเคราะห์สามารถนําไปใช้ในการ วิเคราะห์เชิงลึก (Detailed 
Analysis)ได้ทันที
Key Insight:
การ Join คือสะพานทีÉเชืÉอม ข้อมูลเชิงรายละเอียดจากหลายตาราง มารวมเป็น ตารางเดียว (Single Flat File) เพืÉอรองรับการวิเคราะห์และรายงานในขัÊนตอนถัดไป

3. DATA PIVOTING
42ความหมายของ Data Pivoting
Data Pivoting = การ “หมุน” ข้อมูลจาก แถว (Rows)ให้กลายเป็น คอลัมน์ 
(Columns)
ซอฟต์แวร์บางตัวออกแบบมาให้ทํางานกับ Pivoted Data มากกว่า Unpivoted Data
การเข้าใจโครงสร้างของโปรแกรมปลายทางทีÉใช้วิเคราะห์ข้อมูล จึงเป็นสิÉงสําคัญในการ
ตัดสินใจว่าจะ Pivot ข้อมูลหรือไม่
ความเชืÉอมโยงกับ Aggregation
การ Pivot มักจะ เกีÉยวข้องกับ Aggregation(เช่น การ Sum ข้อมูลตาม Vendor 
และ Product Category)
ข้อจํากัด: เมืÉอ Aggregation เกิดขึÊน → รายละเอียดระดับเดิม (เช่น รายการสินค้า
เดีÉยว) จะ สูญหายและไม่สามารถย้อนกลับมาได้ เว้นแต่จะกลับไปใช้ Disaggregated 
Data ดัÊงเดิม
แนวปฏิบัติทีÉดี (Best Practice)
เก็บข้อมูลในรูปแบบ ละเอียดทีÉสุด (Disaggregated)
ใช้ Pivot & Aggregation เฉพาะในขัÊนตอนการวิเคราะห์/รายงาน
หลีกเลีÉยงการเก็บถาวรในรูปแบบ Pivoted เพราะจะสูญเสียความยืดหยุ่นในการ
วิเคราะห์
ตัวอย่าง  Pivot
(Pivot ครัÊงแรก (Figure 1):
แต่ละแถว = Vendor Name
แต่ละคอลัมน์ = Product Categories
ค่าภายในตาราง = Total Product Costs (Sum)
Pivot กลับ (Figure 2):
ข้อมูลใกล้เคียงกับต้นฉบับมากขึÊน
แต่ยังคง Aggregatedในระดับสูงกว่า
ข้อมูลรายละเอียดของสินค้า (Individual Products) สูญหายไปตัÊงแต่ Pivot แรก → ไม่สามารถกู้คืนได้Figure 1 Figure 2

DATA STANDARIZATION

DATA STANDARDIZATION
44ความหมายของ Data Standardization
Data Standardization = กระบวนการทําให้ โครงสร้าง (Structure)และ ความหมาย 
(Meaning)ของแต่ละข้อมูล (Data Element) เป็นมาตรฐานเดียวกันเพืÉอให้ข้อมูลสามารถ นําไป
วิเคราะห์และใช้ตัดสินใจ ได้อย่างถูกต้อง
ความสําคัญ
มีความสําคัญโดยเฉพาะเมืÉอ รวมข้อมูลจากหลายแหล่ง (Merging Data Sources)
ทําให้ข้อมูลจากระบบทีÉต่างกันสามารถสืÉอสารและเปรียบเทียบกันได้
วิธีการทําให้ข้อมูลเป็นมาตรฐาน (Achieving Standardized Data)
แปลงข้อมูลให้เป็น Format หรือ Data Type เดียวกัน
ใช้ Coding Scheme  เดียวกัน (เช่น Country Code, Currency Code)
ทําให้ข้อมูลอยู่ใน Field ทีÉถูกต้อง และจัดเรียง Field ให้เป็นระบบ
แนวคิดเชิงปฏิบัติ (Practical Perspective)
เมืÉอทํางานกับ Database File  หรือ Flat File Format→คิดว่าเป็นการทําให้ 
Columns ของข้อมูลถูกต้องและเป็นมาตรฐาน
ประเด็นทีÉควรพิจารณา (Considerations in Data Standardization)
1. Data Parsing→การแยกข้อมูลออกจากกัน (เช่น ชืÉอเต็ม 
→ First Name, Last Name)
2. Data Concatenation →การรวมข้อมูลเข้าด้วยกัน (เช่น 
Address Line 1 + Address Line 2)
3. Cryptic Data Values →การใช้ค่าทีÉเข้าใจยาก เช่น Code ทีÉ
ไม่ชัดเจน ต้องแปลงให้เป็นความหมายทีÉเข้าใจได้
4. Misfielded Data Values →ข้อมูลอยู่ใน Field ทีÉผิด (เช่น 
หมายเลขโทรศัพท์ถูกใส่ในช่อง Address)
5. Data Formatting & Consistency →การทําให้รูปแบบข้อมูล
สอดคล้องกัน (เช่น Date Format, Currency Format)

1. DATA PARSING
ความหมาย (Definition)
•Data Parsing = การแยกข้อมูลจาก ฟิลด์เดียว ออกมาเป็น หลายฟิลด์ย่อย
•จุดประสงค์: เพืÉอให้สามารถนํา องค์ประกอบย่อย ๆ ของข้อมูล ไปใช้วิเคราะห์ได้แยกกันอย่างมี
ประสิทธิภาพ
การใช้งาน (Uses)
•Data Parsing = เปลีÉยนข้อมูลทีÉซับซ้อนให้ง่ายต่อการวิเคราะห์
•เมืÉอข้อมูลถูกแยกเป็นองค์ประกอบย่อย → ทําให้สามารถใช้ข้อมูลนัÊนได้ ตรงตามวัตถุประสงค์เชิง
วิเคราะห์ มากขึÊน
45Data Parsing Example

2. DATA CONCATENATION
ความหมาย (Definition)
Data Concatenation = การนําข้อมูลจาก สองฟิลด์หรือมากกว่า มารวมกันเป็น ฟิลด์เดียว
•มักใช้ในการ รวมข้อมูลเพืÉอให้อ่านง่ายขึÊน หรือ สร้างตัวระบุเฉพาะ (Unique Identifier)Key Insight
การใช้งาน (Uses)
•ทําให้ ข้อมูลแสดงผลได้สะดวกขึÊน (เช่น Full Name)
•ใช้สร้าง Unique Identifier สําหรับตารางทีÉ Primary Key ไม่สมบูรณ์
•ใช้ในการรวม Field หลายตัวเพืÉอสร้าง Key สําหรับ Joinระหว่างตาราง
46Data Concatenation Example
Key Takeaway:
•Parsing = แยก Field →หลาย Field
•Concatenation = รวม Field →Field เดียว
•ทัÊงสองเทคนิคมักถูกใช้ร่วมกันใน Data Transformation เพืÉอทําให้ข้อมูล สะอาด (Clean), มีโครงสร้าง (Structured), และพร้อมใช้งานเชิงวิเคราะห์

3. CRYPTIC DATA VALUES
47ความหมาย (Definition)
Cryptic Data Values = ค่าข้อมูลทีÉ ไม่มี
ความหมายชัดเจน หากไม่เข้าใจ Coding 
SchemeทีÉใช้กําหนดค่า
ผู้ใช้จะต้องอ้างอิง Data Dictionary หรือ 
ตาราง Mappingจึงจะเข้าใจค่าทีÉบันทึกไว้
Key Takeaway:
ปัญหา Cryptic Data Values = ทําให้ผู้ใช้ ไม่สามารถตีความข้อมูลได้ทันที
แนวทางแก้ทีÉดีทีÉสุดคือ ใช้ Data Dictionary + Join Data เพืÉอเติมความหมาย  และตัÊงชืÉอฟิลด์ให้เข้าใจง่าย
Best Practiceแนวทางแก้ไข 
(Solution)ปัญหา (Problem) ตัวอย่าง (Example) ประเภท (Type)
ใช้ Join DataเพืÉอเก็บ ID
เดิม และเพิÉมความหมาย
ประกอบ-Replace ด้วยคํา
เต็ม - หรือ Add 
Column ใหม่เพืÉอเก็บ
คําอธิบายผู้ใช้ไม่เข้าใจค่าตัวเลข/รหัส 
หากไม่อ้างอิง Data 
Dictionary-Employee Position: 1 = 
Partner, 2 = Senior Consultant, 
3 = Analyst -ProdCat: 1 = 
Kitchen Appliances, 2 = Fans, 3 
= Hand ToolsCryptic Data 
Values
ตัÊงชืÉอ Field ให้ชัดเจน เช่น 
PreferredVendor แทนทีÉจะ
ใช้ VendorStatusใช้ 0 และ 1 ตาม
มาตรฐาน (0 = 
Absence, 1 = 
Presence)ค่าตัวเลข (0/1) เข้าใจได้ยาก
ถ้า Field Name ไม่สืÉอ
ความหมาย-Preferred Vendor: 1 = Yes, 0 = 
NoDummy Variables 
(Dichotomous)ตาราง: Cryptic Data Values & Dummy Variables

4. MISFIELDED DATA VALUES
48ความหมาย (Definition)
Misfielded Data Values = ค่าข้อมูลทีÉมี รูปแบบถูกต้อง 
(Correctly Formatted) แต่ถูกบันทึกใน Field ทีÉไม่ถูกต้อง
ปัญหาคือ ข้อมูลอยู่ผิดทีÉ ไม่ใช่ว่าข้อมูลมีรูปแบบผิด
ระดับปัญหา 
(Level)การแก้ไข (Correction) ตัวอย่าง (Example) ลักษณะปัญหา (Issue)
Individual Rowย้ายค่า “Germany” ไป 
Field CountryField: City = “Germany” 
(ควรอยู่ใน Country)ค่าข้อมูลอยู่ผิด Field
Entire Columnสลับข้อมูลให้ตรง Field ทีÉ
ถูกต้องField: City เก็บ State 
Field: State เก็บ CityทัÊง Column อยู่ผิด Field
Individual Rowย้ายค่ากลับไป Field 
Phoneตัวเลขโทรศัพท์ถูกบันทึกใน 
Field Addressค่าข้อมูลถูกต้อง แต่ Fieldผิด
Individual Rowย้าย “Bangkok” ไป Field 
CityField: PostalCode = 
“Bangkok”รูปแบบข้อมูลไม่ผิด แต่ Field 
ผิดKey Takeaway:
Misfielded Data Values = Data ถูก แต่ทีÉผิด
อาจเกิดได้ทัÊง ระดับแถว (Row)และ ระดับคอลัมน์ (Column)
ต้องอาศัย Data Dictionary / Schema เพืÉอค้นพบและแก้ไขให้ถูกต้อง

5. DATA FORMATTING AND DATA CONSISTENCY
49ตัวอย่าง ข้อมูลทีÉมี format ต่างกัน ความหมาย (Definition)
•Data Formatting = รูปแบบการแสดงข้อมูล เช่น วันทีÉแสดงเป็น “03/04/82” หรือ “April 3, 1982”
•Data Consistency = ความสมํÉาเสมอในการเก็บและใช้ข้อมูล → ทุกค่าต้องถูกเก็บในรูปแบบเดียวกัน
Data Consistency Data Formatting หัวข้อ
วิธีการ เก็บข้อมูลจริง (Storage Format) วิธีการ แสดงผลข้อมูล (Display Format) ความหมาย (Definition)
วันทีÉเก็บเป็น - Excel Serial Date = 29170.0416 -Unix Epoch Time = 311130000 วันทีÉเดียวกันแสดงเป็น - April 3, 1982 -03/04/82 -04/03/82 ตัวอย่าง (Example)
- ข้อมูล ไม่สอดคล้องกันระหว่างระบบ - การวิเคราะห์ผิดเพราะเก็บ หลายรูปแบบผสมกัน - ทําให้ ผู้ใช้สับสน - อาจทําให้ข้อมูล ผิดเพีÊยนตอน Import/Export ปัญหา (Problem)
ระบบเก็บวันทีÉบางส่วนเป็น Serial Date และบางส่วนเป็น Epoch Time ใน Field เดียวกันMI5: Spreadsheet Error →เลขโทรศัพท์ 134 หมายเลขท้ายถูก
เปลีÉยนเป็น “000”กรณีศึกษา (Case Study)
- เก็บข้อมูลด้วย Format เดียวกัน (เช่น UTC สําหรับเวลา) - หลีกเลีÉยงการสลับไปมา
ระหว่างวิธีเก็บหลายแบบ- เลือก Format เดียว ใช้ทัÊง Field/File -กําหนด Format ไว้ใน Data
DictionaryBest Practice
Key Takeaway: ทัÊงสองอย่างต้องควบคู่กัน → เพืÉอความถูกต้องและน่าเชืÉอถือของข้อมูล
Formatting = ความสมํÉาเสมอในการ “แสดงผล”
Consistency = ความสมํÉาเสมอในการ “เก็บข้อมูล”ตารางเปรียบเทียบ Data Formatting vs Data Consistency

DATA CLEANING

DIRTY DATA AND DATA CLEANING
ปัญหาหลัก
•ข้อมูลสกปรก (Dirty Data) = ข้อมูลทีÉ ไม่สอดคล้องกัน / ไม่ถูกต้อง / ไม่สมบูรณ์
•พบมากทีÉสุดจากการสํารวจผู้ใช้งาน Kaggle 16,000 คน
ความเสีÉยงและต้นทุน
•ทําให้การตัดสินใจผิดพลาด
•สูญเสียชืÉอเสียงและความน่าเชืÉอถือ
•ตัวอย่าง: Fidelity Magellan Fund
•รายงานปันผล $4.32 ต่อหุ้น → ผิดพลาด
•นักบัญชีภาษีลืมใส่เครืÉองหมายลบ (ขาดทุน 1.3 พันล้านดอลลาร์)ระบบตีความเป็นกําไร 
→ เกิดความคลาดเคลืÉอน 2.6 พันล้านดอลลาร์ข้อผิดพลาดทีÉพบบ่อย
รูปแบบข้อมูลไม่ตรงกัน (วันทีÉ, หน่วย, ตัวพิมพ์)
ข้อมูลหาย/ไม่สมบูรณ์
ข้อมูลซํÊาซ้อน
ค่าผิดปกติ (Negative Age, Impossible Dates)
รหัสหมวดหมู่ไม่ถูกต้อง (NY N.Y. New York)
ความผิดพลาดจากการป้อนข้อมูล (decimal, transposed 
digits, missing sign)
51

DATA QUALITY ISSUE
52
แนวทางแก้ไข ตัวอย่างจาก S&S คํานิยาม ประเภทปัญหา
ตรวจสอบว่าไม่ใช่คําสัÉงซืÊอจริงซํÊา แล้ว
ลบข้อมูลซํÊาออกแถวทีÉ 17 เป็นข้อมูลซํÊากับแถวทีÉ 6 การลบระเบียนทีÉมีค่าข้อมูลเหมือนกันทุกช่องData De-Duplication
(การลบข้อมูลซํÊา)
กรองเฉพาะหมวดที ÉเกีÉยวข้อง จัดการ
ค่าทีÉหายด้วยการเก็บไว้, ลบ หรือทํา 
Data Imputationไฟล์มี หมวดสินค้า 3 (เครืÉองมือช่าง) ทีÉไม่เกีÉยวข้องกับการ
วิเคราะห์การลบระเบียน/คอลัมน์ทีÉไม่เกีÉยวข้อง หรือจัดการ
ค่าทีÉหาย (null)Data Filtering (การ
กรองข้อมูล)
ตรวจสอบจาก log ระบบ หรือยืนยันกับ 
vendor แล้วแก้ให้สอดคล้องกันหมายเลขโทรศัพท์ของ Milton Armstrong ไม่ตรงกัน (แถว 16 
เทียบกับแถวอืÉน)ข้อมูลสิÉงเดียวกันแต่ไม่สอดคล้องกันContradiction Errors
(ข้อมูลขัดแย้ง)
แก้ไขข้อมูลให้ตรงตาม threshold ทีÉ
กําหนด (10 หลัก)ฟิลด์ PhoneNumber ควรมี 10 หลัก แต่แถวทีÉ 13 มี 13 หลักค่าอยู่นอกช่วงที Éกําหนดหรือไม่ตรงตามรูปแบบ
ฟิลด์Threshold Violations
(เกินค่าทีÉกําหนด)
แก้ไขรหัสไปรษณีย์เป็น 28027 ให้
ถูกต้องสําหรับ Concord, NCเมือง Concord, NC ใส่รหัสไปรษณีย์ของ Concord, CA 
(94519)ค่าของคุณลักษณะรองไม่ตรงกับคุณลักษณะหลักAttribute 
Dependency Errors
(ความสัมพันธ์ผิดพลาด)
แก้ไขการสะกดให้ถูกต้องเป็น “San 
Diego”เมือง San Diegoพิมพ์ผิดเป็น “SanDgieo” (แถว 6, 8, 17)ความผิดพลาดจากการกรอกข้อมูล เช่น พิมพ์ผิด, 
ตัวเลขสลับ, ข้อมูลหายData Entry Errors
(การป้อนข้อมูลผิด)
Example : Duplicate

DATA VALIDATION

DATA VALIDATION
54การตรวจสอบความถูกต้องของข้อมูล (Data 
Validation)
ความหมาย
•กระบวนการวิเคราะห์ว่าข้อมูลมีคุณสมบัติเป็น 
ข้อมูลทีÉมีคุณภาพสูง หรือไม่
•ใช้ทัÊง แบบเป็นทางการ  (หลังการแปลงข้อมูล) 
และ แบบไม่เป็นทางการ  (ตรวจสอบระหว่าง
การแปลงข้อมูล)
•เป็นขัÊนตอนสําคัญก่อนการทํา Data 
Cleaning และมักทําแบบวนซํÊา (Iterative 
Process) เพืÉอทัÊงค้นหาปัญหาทีÉต้องแก้ไข และ
ยืนยันว่าการแปลงข้อมูลสําเร็จและถูกต้อง
จุดแข็ง ตัวอย่าง วิธีการ Technique
เร็ว ง่าย ใช้ได้ทัÊงข้อมูลเล็กและใหญ่ตรวจพบ “Milton Armstrong” มี
หลายรูปแบบการสะกด หรือเบอร์
โทรศัพท์ผิดรูปแบบใช้สายตามองหาความผิดปกติ เช่น 
การเรียงข้อมูล, การหาค่าซํÊา (unique 
values)Visual Inspection (การตรวจสอบ
ด้วยสายตา)
ช่วยตรวจหาค่าผิดปกติและการสูญ
หายของข้อมูลราคาสินค้าติดลบ, ยอดรวมหลังแปลง
ไม่ตรงกับต้นทางคํานวณสถิติเบืÊองต้น (min, max, 
mean, median, sum) และ
เปรียบเทียบก่อน–หลังการแปลงBasic Statistical Tests (การ
ทดสอบทางสถิติพืÊนฐาน)
ใช้ประเมินคุณภาพโดยรวมและ
คํานวณ Error Rate ได้Dataset 100,000 ระเบียน → 
ตรวจ 1,000 ระเบียน → พบ 
error rate 7%เลือกสุ่มระเบียนมาวิเคราะห์และ
ตรวจสอบกับข้อมูลต้นทางAudit a Sample (ตรวจสอบ
ตัวอย่างข้อมูล)
ให้ความเชืÉอมัÉนสูง เหมาะกับข้อมูลทาง
ธุรกิจสําคัญตรวจสอบว่า Debit = Credit, Sub-
ledger = GL, จํานวน × ราคา = 
ยอดรวมใช้ Business Rules หรือความรู้
เฉพาะด้านเพืÉอตรวจสอบAdvanced Testing (การทดสอบขัÊน
สูง)ตาราง สรุปการตรวจสอบความถูกต้องของข้อมูล (Data Validation Techniques)

EXAMPLE OF TRANSFORM DATA
55

DATA ANALYSIS AND 
PRESENTATION

TEST CASE
57Case Study: TD Bank Group –Building an Analytics 
Mindset
Case
TD Bank Group ( ธนาคารชัÊนนําของแคนาดาและอเมริกาเหนือ)
ใช้เวลา 5 ปีสร้าง Data Lake
ลงทุนใน เครืÉองมือและการฝึกอบรม  ให้พนักงานวิเคราะห์ข้อมูลเองได้
 Actions
สร้างระบบให้พนักงานเข้าถึงข้อมูลโดยตรง (ไม่ต้องพึÉง Data Science Team)
จัดหาเครืÉองมือ Self-service Analytics & Visualization
อบรมให้พนักงานทุกระดับสามารถทํา Data Analysis ได้
Results
•
+90% ประสิทธิภาพโครงการวิเคราะห์
•
-60% ต้นทุนการจัดการข้อมูล
•
-30% ข้อร้องเรียนจากลูกค้าซํÊาซ้อน
Lesson Learned
ETL อย่างเดียว ไม่พอ→ ต้อง วิเคราะห์ และ สืÉอสารผลลัพธ์
Analytics Mindset ต้องครอบคลุม:
การเลือกใช้ เทคนิควิเคราะห์ข้อมูลทีÉเหมาะสม
การ ตีความและสืÉอสารผลลัพธ์กับผู้มีส่วนได้ส่วนเสีย

DATA ANALYSIS
58ประเภทของการวิเคราะห์ข้อมูล (Categories of Data Analytics)
มีการแบ่งการวิเคราะห์ข้อมูลออกเป็น 4 ประเภท ได้แก่ Descriptive, Diagnostic, Predictive และ Prescriptiveโดยแต่ละประเภทมีระดับความซับซ้อนและ
คุณค่าทีÉเพิÉมให้แก่องค์กรแตกต่างกัน การเป็นผู้เชีÉยวชาญในแต่ละด้านจําเป็นต้องอาศัยการฝึกฝนและเรียนรู้เพิÉมเติม
ตัวอย่างการใช้งาน วิธีการ/เทคนิค คําถามหลัก ประเภท
- อัตรากําไรขัÊนต้น - Inventory 
turnover -Budget vs. Actual- สถิติพืÊนฐาน (mean, median, 
variance) -Visualization (charts, 
dashboards)เกิดอะไรขึÊน? (What happened? )Descriptive Analytics
(การวิเคราะห์เชิงพรรณนา)
- วิเคราะห์สาเหตุทีÉกําไรขัÊนต้นลดลง - 
ตรวจสอบสัดส่วนสินค้า/กลยุทธ์
การตลาด-Root cause analysis -“5 
Why’s” -Hypothesis testingทําไมสิÉงนีÊถึงเกิดขึÊน? (Why did it 
happen?)Diagnostic Analytics
(การวิเคราะห์เชิงวินิจฉัย)
- พยากรณ์ยอดขาย - การคาดการณ์
ความเสีÉยงด้านเครดิต - Predictive
maintenance-Regression analysis -Machine 
learning models -ForecastingสิÉงใดน่าจะเกิดขึÊน? (What is
likely to happen? )Predictive Analytics
(การวิเคราะห์เชิงทํานาย)
- แนะนํากลยุทธ์การตัÊงราคา - กําหนด
เส้นทางการขนส่งทีÉดีทีÉสุด - การ
จัดสรรทรัพยากร-Optimization models -
Simulation -Scenario analysisควรทําอย่างไรต่อไป? (What 
should we do? )Prescriptive Analytics
(การวิเคราะห์เชิงกําหนด)

DATA ANALYSIS
59ปัญหาทีÉพบบ่อยในการทํา Data Analytics
1.GIGO (Garbage In, Garbage Out)
•หากข้อมูลไม่คุณภาพ → ผลการวิเคราะห์ไม่มีค่า
2.Overfitting
•โมเดลตรงกับข้อมูลเก่าเกินไป → ทํานายข้อมูลใหม่ไม่ได้
3.Extrapolation เกินช่วงข้อมูล
•ใช้โมเดลเกินขอบเขตข้อมูลเดิม → ผลลัพธ์ผิดเพีÊยน (เช่น โมเดลบ้าน 2,000–3,000 ตร.ฟุต → ไปใช้ทํานายบ้าน 7,000 
ตร.ฟุต)
4.Variation (ความแปรปรวน)
•ไม่มีโมเดลใดทํานายได้แม่นยํา 100% → ควรรายงานเป็นช่วง (เช่น อุณหภูมิ 75–85 องศา ทีÉความมัÉนใจ 95%) ไม่ใช่ค่าจุด
เดียว

1. DESCRIPTIVE ANALYTICS
602. เทคนิคสําคัญใน Descriptive Analytics
Central Tendency ( แนวโน้มเข้าสู่ศูนย์กลาง)
ค่าเฉลีÉย (Mean) และค่ามัธยฐาน (Median)
ใช้เปรียบเทียบเพืÉอดูว่ามี Outlier หรือไม่
Spread (การกระจาย)
ช่วงของข้อมูล (Range)
ส่วนเบีÉยงเบนมาตรฐาน ( Standard Deviation)
ค่าควอไทล์ (Quartiles)
Distribution ( การกระจายตัว)
ตรวจสอบรูปแบบการกระจาย เช่น การกระจายปกติ (Normal Distribution)
ใช้เพืÉอเลือกสถิติทีÉเหมาะสมในการวิเคราะห์
Correlation (ความสัมพันธ์)
วัดด้วยค่าสหสัมพันธ์ (Correlation Coefficient) ระหว่าง 1 ถึง +1
แสดงการเคลืÉอนไหวของตัวแปรทีÉสัมพันธ์กัน แต่ไม่เท่ากับเหตุและผล 
(Correlation Causation)
การแสดงผลด้วย Visualization (Viz) เช่น กราฟ แผนภาพ หรือแอนิเมชัน มักช่วยให้เข้าใจได้เร็วและชัดเจนขึÊน
การวิเคราะห์ข้อมูลเชิงคุณภาพข้อมูลเช่น โพสต์ในโซเชียลมีเดีย สามารถแปลงเป็นข้อมูลเชิงปริมาณได้ (เช่น นับจํานวนข้อความเชิงบวก/เชิงลบ หรือใช้ Text Analysis) เพืÉอให้
สามารถวิเคราะห์ต่อได้เหมือนข้อมูลเชิงปริมาณ1. Descriptive Analytics ( การวิเคราะห์เชิงพรรณนา)
คําถามหลัก: “เกิดอะไรขึÊน?”
ใช้เทคนิค Exploratory Data Analysis (EDA) เพืÉอสํารวจข้อมูลโดยไม่อิง
สมมติฐานหรือโมเดลทีÉซับซ้อน
ตัวอย่างการใช้งาน:
ผู้ตรวจสอบบัญชีภายนอกคํานวณ อัตรากําไร, leverage ratios เพืÉอดู
ความเสีÉยงทางธุรกิจและระบุการทุจริต
นักบัญชีองค์กรคํานวณ ต้นทุนต่อหน่วย, inventory turnover,
customer acquisition cost, variance budget-to-actual เพืÉอ
ติดตามประสิทธิภาพการดําเนินงาน

2. DIAGNOSTIC ANALYTICS
612. Diagnostic Analytics ( การวิเคราะห์เชิงวินิจฉัย)
คําถามหลัก: “ทําไมสิÉงนีÊจึงเกิดขึÊน?”
สร้างต่อจาก Descriptive Analytics โดยเน้นหาสาเหตุของเหตุการณ์
แบ่งเป็น 2 แบบ:
แบบไม่เป็นทางการ ( Informal Analysis): ใช้ตรรกะและสถิติพืÊนฐาน เช่น
หากกําไรขัÊนต้นลดลง → ตรวจสอบสัดส่วนสินค้า → พบว่าสินค้ามาร์จินตํÉาขายเพิÉมขึÊนเพราะการตลาดโฆษณามาก
ใช้หลักการ “5 Why’s” →ถาม “ทําไม?” ซํÊาหลายครัÊงจนกว่าจะเจอสาเหตุทีÉแท้จริง
แบบเป็นทางการ ( Formal/Confirmatory Analysis): ใช้ Hypothesis Testing เพืÉอตรวจสอบความสัมพันธ์หรือสมมติฐาน โดยกระบวนการคือ:
ระบุสมมติฐานศูนย์ (Null) และสมมติฐานทางเลือก (Alternative)
กําหนดระดับนัยสําคัญ (Significance Level)
เก็บตัวอย่างข้อมูลและคํานวณค่า P-value
เปรียบเทียบ P-value กับระดับนัยสําคัญ → ตัดสินใจว่าปฏิเสธหรือยอมรับสมมติฐาน
สมมติฐานควรเขียนเป็น ข้อความทีÉทดสอบได้ ไม่ใช่คําถาม เช่น:“หากเราจ่ายค่าตอบแทนพนักงานเพิÉมขึÊน พนักงานจะมีแนวโน้มลาออกน้อยลง”

3. PREDICTIVE ANALYTICS
623. Predictive Analytics ( การวิเคราะห์เชิงทํานาย)
คําถามหลัก: “สิÉงใดมีแนวโน้มจะเกิดขึÊนในอนาคต?”
ใช้ข้อมูลในอดีตเพืÉอสร้าง รูปแบบ (Patterns)และคาดการณ์เหตุการณ์ในอนาคต
ตัวอย่างการใช้งาน:
Amazon→ใช้พฤติกรรมการค้นหา/การซืÊอ เพืÉอแนะนําสินค้า
Match.com→ใช้อัลกอริทึมทํานายความเข้ากันได้ของคู่รัก
Boston Medical Center →ใช้ “Hospital IQ” คาดการณ์ความต้องการผู้ป่วย → ปรับแผน
บุคลากรและบริการสุขภาพ
ขัÊนตอนการสร้าง Predictive Model
1.เลือกตัวแปรเป้าหมาย (Target Variable)
•Categorical (เช่น “ลูกค้าจะซืÊอ/ไม่ซืÊอ”)
•Numeric (เช่น “ลูกค้าจะใช้จ่ายเท่าไหร่”)
2.เตรียมข้อมูลทีÉเหมาะสม (ETL)
•ข้อมูลยิÉงหลากหลาย ยิÉงมีประโยชน์ต่อการทํานาย
3.สร้างและตรวจสอบโมเดล
•แบ่งข้อมูลเป็น Training Set และ Test Set→ป้องกันปัญหา Overfitting
•วิธีทีÉใช้:
Regression (Linear, Polynomial, Regression Trees) →สําหรับตัวเลข
Classification (Logistic Regression, Random Forests, Decision Trees, KNN, 
SVM) →สําหรับกลุ่ม/ประเภทประเด็นสําคัญ
Overfitting:โมเดลแม่นกับข้อมูลเก่า แต่ทํานายข้อมูลใหม่ไม่ได้ → ต้อง
ใช้ Test Set ตรวจสอบ
Model Update: ความสัมพันธ์ของข้อมูลเปลีÉยนแปลง (เช่น E-commerce 
→Online Shopping) →โมเดลต้องอัปเดตอย่างต่อเนืÉอง
Machine Learning: ระบบเรียนรู้และปรับปรุงโมเดลโดยอัตโนมัติ เช่น การ
ทํานายเครดิตลูกค้า

4. PRESCRIPTIVE ANALYTICS
634. Prescriptive Analytics (การวิเคราะห์เชิงกําหนด)
คําถามหลัก: “ควรทําอะไรต่อไป?”
ไม่ใช่แค่ทํานาย แต่ยังเสนอแนวทางปฏิบัติหรือการตัดสินใจอัตโนมัติ
ตัวอย่างการใช้งาน:
UPS → พัฒนา Prescriptive Model สําหรับเส้นทางส่งพัสดุ → ลดระยะทาง, ประหยัดเวลา, ลดการปล่อยคาร์บอน, ประหยัดต้นทุน $300–400 ล้าน/ปี
ใช้เทคนิค: AI, Machine Learning, Optimization, Simulation

DATA PRESENTATION
64การแสดงผลข้อมูล (Data Visualization)
Choosing the Right Visualization
เหตุผลทีÉการแสดงผลข้อมูลสําคัญ
สํานวนทีÉว่า “ภาพหนึÉงภาพแทนคํานับพันคํา” สอดคล้องกับงานวิจัยทีÉยืนยันว่ามนุษย์ถูก
ออกแบบให้ ประมวลผลข้อมูลเชิงภาพได้ดีกว่าข้อมูลทีÉเป็นตัวหนังสือ โดยประโยชน์ของ Data 
Visualization ได้แก่:
ประมวลผลเร็วกว่า  การอ่านข้อมูลในตารางหรือตัวอักษร
ใช้งานง่ายกว่า ผู้ใช้ไม่ต้องการคําอธิบายมากในการค้นหาข้อมูล
สอดคล้องกับรูปแบบการเรียนรู้หลักของคนส่วนใหญ่ (เป็น Visual Learners)
ตัวอย่างจากธุรกิจจริง
บริษัทขายตรงแห่งหนึÉงลงทุนสร้างโมเดลคาดการณ์ว่าลูกค้าในพืÊนทีÉใดมีแนวโน้มจะซืÊอสินค้ามาก
ทีÉสุด แต่เมืÉอส่งผลการวิเคราะห์เป็นรายงานยาว ๆ ให้ทีมขาย → ไม่ช่วยให้ขายได้จริง
เมืÉอบริษัทเปลีÉยนมานําเสนอข้อมูลเป็น แผนทีÉทีÉมี Layer สี แสดงโอกาสหาลูกค้าใหม่ → ทีม
ขายเข้าใจทันทีว่าควรไปโฟกัสทีÉไหนส่งผลให้มีแรงจูงใจมากขึÊนและสร้างผลลัพธ์ได้ทันที
ข้อดี / จุดเด่น ตัวอย่างการใช้งาน Visualization ทีÉเหมาะสม Purpose
เห็นความแตกต่างระหว่างกลุ่ม
ชัดเจน ใช้ง่ายเปรียบเทียบค่าตอบแทนระหว่างเพศ - 
ความพึงพอใจ vs ผลการทํางาน - 
Performance ปีนีÊ vs ปีทีÉแล้วBar Chart, Column 
Chart, Bullet GraphComparison
(การเปรียบเทียบ)
แสดงให้เห็นความสัมพันธ์ของตัว
แปรเชิงปริมาณ 2 ตัววิเคราะห์ความสัมพันธ์ระหว่างราคาสินค้า
และยอดขาย –ความสัมพันธ์ระหว่าง
ค่าธรรมเนียมล่าช้าและการชําระตรงเวลาScatterplot (+ 
Regression Line)Correlation
(ความสัมพันธ์)
ระบุ Outliers และรูปแบบการ
กระจายของข้อมูลได้วิเคราะห์คะแนนสอบของนักเรียนทัÊงห้อง –
การกระจายของรายได้ลูกค้าHistogram, BoxplotDistribution
(การกระจายตัว)
เห็นทิศทาง/การเปลีÉยนแปลงของ
ข้อมูลตามกาลเวลาแนวโน้มยอดขายรายเดือน –จํานวน
ผู้ใช้งานแอปพลิเคชันต่อวันLine Chart, Area ChartTrend 
Evaluation(การ
ดูแนวโน้มตาม
เวลา)
เข้าใจโครงสร้างส่วนประกอบ
ภายในทัÊงหมดได้ทันที สัดส่วนรายได้ตามหน่วยธุรกิจ –ส่วนแบ่ง
ตลาดของคู่แข่งPie Chart, Stacked Bar, 
TreemapPart-to-Whole
(ส่วนต่อทัÊงหมด)ตาราง Cheat Sheet สรุป 5 วัตถุประสงค์หลักของ Data Visualization

DATA PRESENTATION
65ตาราง Visualization Purposes and Types
 Usefulness (ข้อดี) Examples (ตัวอย่าง) Charts ทีÉเหมาะสม Purpose
เข้าใจความแตกต่างระหว่างกลุ่ม/
หมวดหมู่ชัดเจน-Gender pay gap -Job satisfaction vs 
performance -Performance year vs yearBar Chart, Column 
Chart, Bullet GraphComparison(การ
เปรียบเทียบ)
เห็นความสัมพันธ์เชิงสถิติของตัวแปร 
2 ตัว-Performance vs income -Job satisfaction 
vs performance -Training vs performance 
(levels)Scatterplot, Heatmap Correlation(ความสัมพันธ์)
ระบุ Outliers และรูปแบบการ
กระจายของข้อมูลได้-Salary distribution (bins of $10,000) -
Performance rating distribution -Salary by 
departmentHistogram, BoxplotDistribution(การกระจาย
ตัว)
แสดงการเปลีÉยนแปลงตามกาลเวลา 
เข้าใจทิศทาง-Annual compensation change -Total 
compensation by department (per year)Line Chart, Area 
ChartTrend Evaluation
(แนวโน้มตามเวลา)
เข้าใจสัดส่วนองค์ประกอบของ
ทัÊงหมดได้ชัด-% pay by department -Degree mix 
contributionPie Chart, TreemapPart-to-Whole (ส่วนต่อ
ทัÊงหมด)
ใช้เมืÉอเป้าหมายซับซ้อน เช่น 
Spatial, Flow, หรือข้อมูลละเอียด-Data overlays on maps -Flow diagrams 
-Detailed reports in tablesMaps, Sankey 
Diagram, Combo 
Chart, TablesOther(อืÉน ๆ)Data Visualization – Purposes & Examples

DATA PRESENTATION
66Designing High-Quality Visualizations
3 หลักการออกแบบ Visualization ทีÉมีคุณภาพสูง
1. Simplification ( การทําให้ง่ายขึÊน)
ความหมาย: ทําให้ Visualization เข้าใจง่าย ตรงประเด็น ลดความซับซ้อนทีÉไม่จําเป็น
แนวทาง:
Title →กระชับ ชีÊ insight โดยตรง
Axes →ตัด tick mark/เส้น/gridline ทีÉเกินความจําเป็น
Legend →แสดงเฉพาะ series สําคัญ หรือเขียน label บนกราฟเลย
Data Area →ใช้กราฟทีÉเหมาะสม, สีไม่เกิน 4–5 สี, หลีกเลีÉยง 3D/เอฟเฟกต์รบกวน
2. Emphasis ( การเน้นประเด็นสําคัญ)
•ความหมาย: ทําให้ผู้ดูเห็น message หลักได้ทันที
•แนวทาง:
•Title →ใช้ข้อความเชิง insight เช่น “Q4 Sales Increased 20%”
•Axes →ใช้ scale ทีÉเหมาะสม ไม่บิดเบือน และช่วยเน้น pattern
•Legend →ใช้สีหรือการจัดลําดับเพืÉอเน้น series ทีÉสําคัญ
•Data Area →ใช้การเน้นสี, ขนาด, หรือ annotation highlight จุดสําคัญ3. Ethical Presentation ( การนําเสนออย่างมีจริยธรรม)
ความหมาย: หลีกเลีÉยงการทํา Visualization ทีÉทําให้เข้าใจผิดโดยตัÊงใจหรือไม่ตัÊงใจ
แนวทาง:
Title →ไม่ใช้ข้อความทีÉชีÊนําเกินจริง
Axes →ไม่ตัดแกน (axis truncation) จนทําให้การเปลีÉยนแปลงดูเว่อร์เกินจริง
Legend →ใช้สีทีÉเป็นกลาง ไม่ทําให้ข้อมูลถูกตีความผิด
Data Area →ใช้มาตราส่วนทีÉถูกต้อง, ไม่ใช้การบิดเบือน perspective (เช่น Pie 
3D ทีÉหลอกตา)
สรุป
Visualization ทีÉดีต้องมี 3 สิÉงครบ:
•Simplification→เข้าใจง่าย ไม่รก
•Emphasis→เห็นประเด็นสําคัญชัดเจน
•Ethical Presentation →ไม่บิดเบือนหรือทําให้ผู้ชมเข้าใจผิด

DATA PRESENTATION
67ตัวอย่าง GRAPH ทีÉดูได้ยาก

PRINCIPLES OF HIGH-QUALITY VISUALIZATION
68
Example of Simplifying a Visualization Using Distance

69PRINCIPLES OF HIGH-QUALITY VISUALIZATION
เทคนิคหลัก 3 แบบในการสร้าง Emphasis
1.Highlighting ( การไฮไลต์/เน้นสี)
1.ใช้สี, ความต่าง (contrast), label, font, call-out, หรือแม้กระทัÉงลูกศร
เพืÉอดึงความสนใจ
2.มักใช้ใน Data Area(เช่น บาร์, เส้น, พาย) มากทีÉสุด
3.สี เป็นเครืÉองมือทีÉทรงพลังทีÉสุดในการเน้นข้อมูล
2.Weighting ( การให้ความสําคัญด้วยขนาด/นํÊาหนัก)ใช้ความหนาของเส้น, ความเข้มของ
สี, หรือขนาดของวัตถุ เพืÉอบอกว่าส่วนไหนสําคัญกว่าเช่น กราฟเส้นแนวโน้มอาจทํา “เส้นค่า
เป้าหมาย (Target Line)” หนากว่าเส้นอืÉน
3.Ordering (การจัดลําดับ)
การจัดลําดับข้อมูลสามารถช่วยเน้นยํÊา insight ได้
เช่น การเรียงแท่งกราฟจากมากไปน้อย → ทําให้เห็นแผนกทีÉมียอดสูงสุดทันที
1.ซ้าย: ใช้สีเน้น “แผนก Home Electronics” →ผู้ใช้หาได้ทันที เหมาะถ้าต้องการเน้นเฉพาะแผนกนีÊ
2.กลาง: ใช้สีเทาเหมือนกันหมด → ผู้ใช้ต้องตีความเองว่าอะไรสําคัญ → อาจพลาดสารที Éต้องการสืÉอ
3.ขวา: ใช้สีหลายสีสําหรับแต่ละแผนก → เหมาะถ้าอยากเปรียบเทียบแผนก แต่ไม่ช่วยเน้นเฉพาะจุด

70PRINCIPLES OF HIGH-QUALITY VISUALIZATION
Data Deception ( การหลอกลวงด้วยข้อมูล) คือ “การนําเสนอข้อมูลในรูปแบบกราฟ โดยตัÊงใจหรือไม่ตัÊงใจก็ตาม ทีÉทําให้ผู้ชมเข้าใจผิดหรือสร้างความเชืÉอทีÉไม่ตรงกับความจริงของข้อมูล”
หลักปฏิบัติเพืÉอเลีÉยงการหลอกลวงข้อมูล
1.แสดงข้อมูลตามสัดส่วนจริง (Proportional Representation)
1.ตัวเลขทีÉนําเสนอควรสะท้อนค่าทีÉแท้จริง
2.ตัวอย่าง: เริÉมแกน Y จากศูนย์ (0)→ ช่วยหลีกเลีÉยงการขยายความ
ต่างให้ดูมากเกินจริง
2.ใน Visualization ทีÉแสดงแนวโน้ม (Trends)
1.ควรให้ เวลา (Time)บนแกน X แสดงจากซ้ายไปขวา
2.สอดคล้องกับธรรมชาติของการอ่านข้อมูลและหลีกเลีÉยงการสร้างความ
สับสน
3.นําเสนอข้อมูลอย่างครบถ้วนตามบริบท (Present Complete Data)
1.ไม่ตัดข้อมูลทีÉสําคัญออกไปจนทําให้สารผิดเพีÊยน
เช่น การเลือกแสดงเฉพาะบางปี บางช่วง หรือบางกลุ่ม โดยไม่ชีÊแจง → อาจ
ทําให้ผู้ชมเข้าใจผิด
•แสดงเหมือนกับว่า Lucas Median มีความพึง
พอใจในงานลดลงเรื Éอย ๆ ตามกาลเวลาซึ Éงไม่
ถูกต้อง เพราะแกนเวลา จัดเรียงกลับด้านปัญหาคือ ผู้ใช้ส่วนใหญ่คาดหวังว่าเวลา ต้อง
เรียงจากซ้าย →ขวา (เก่า →ใหม่)
การกลับทิศเวลา ทําให้ผู้ชมสับสแสดงเวลาในรูปแบบที Éผู้คนคุ้นเคย: อดีต
อยู่ทางซ้าย →ปัจจุบันอยู่ทางขวา
การนําเสนอแบบนีÊช่วย ลดความเข้าใจ
ผิด และทําให้ผู้ชมตีความได้ถูกต้อง

THANK YOU 
Manatchaya Sriphanlam